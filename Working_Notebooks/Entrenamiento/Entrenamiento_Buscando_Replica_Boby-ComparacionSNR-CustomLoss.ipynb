{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.layers import Conv1D , Dropout , Flatten , MaxPooling1D, Dense, Input, BatchNormalization , Dropout\n",
    "from keras.layers.core import Lambda\n",
    "from keras.models import Model , load_model , model_from_json\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import h5py\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pruebas matemáticas para probar los cálculos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = K.random_uniform(shape = (5,1), minval=0.0, maxval=50.0, dtype=None, seed=None)\n",
    "true_error = K.random_uniform(shape = (5,1), minval=0.0, maxval=1.0, dtype=None, seed=None)\n",
    "pred = K.random_uniform(shape = (5,1), minval=0.0, maxval=50.0, dtype=None, seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true:  tf.Tensor(\n",
      "[[ 9.449398 ]\n",
      " [24.156666 ]\n",
      " [13.77474  ]\n",
      " [ 0.8261502]\n",
      " [40.93929  ]], shape=(5, 1), dtype=float32) \n",
      " ------------------- \n",
      " pred:  tf.Tensor(\n",
      "[[39.02463   ]\n",
      " [20.839184  ]\n",
      " [27.977121  ]\n",
      " [38.462723  ]\n",
      " [ 0.75920224]], shape=(5, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"true: \", true ,\"\\n\",\"-------------------\",\"\\n\", \"pred: \", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta:  tf.Tensor(\n",
      "[[-29.575233]\n",
      " [  3.317482]\n",
      " [-14.202381]\n",
      " [-37.636574]\n",
      " [ 40.180088]], shape=(5, 1), dtype=float32) \n",
      " ------------------- \n",
      " delta_square:  tf.Tensor(\n",
      "[[ 874.69446 ]\n",
      " [  11.005687]\n",
      " [ 201.70763 ]\n",
      " [1416.5117  ]\n",
      " [1614.4395  ]], shape=(5, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"delta: \" , true - pred ,\"\\n\",\"-------------------\",\"\\n\", \"delta_square: \" , K.square(true - pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_error:  tf.Tensor(\n",
      "[[0.15048528]\n",
      " [0.43249726]\n",
      " [0.77380157]\n",
      " [0.46415055]\n",
      " [0.5773468 ]], shape=(5, 1), dtype=float32) \n",
      " ------------------- \n",
      " square_error:  tf.Tensor(\n",
      "[[0.02264582]\n",
      " [0.18705389]\n",
      " [0.5987689 ]\n",
      " [0.21543573]\n",
      " [0.33332932]], shape=(5, 1), dtype=float32) \n",
      " ------------------- \n",
      " ln(true_error):  tf.Tensor(\n",
      "[[-1.89389  ]\n",
      " [-0.8381793]\n",
      " [-0.2564398]\n",
      " [-0.7675463]\n",
      " [-0.5493122]], shape=(5, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"true_error: \" ,true_error,\"\\n\",\"-------------------\",\"\\n\", \n",
    "      \"square_error: \" ,K.square(true_error),\"\\n\",\"-------------------\",\"\\n\", \n",
    "      \"ln(true_error): \" ,K.log(true_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_square:  tf.Tensor(\n",
      "[[ 874.69446 ]\n",
      " [  11.005687]\n",
      " [ 201.70763 ]\n",
      " [1416.5117  ]\n",
      " [1614.4395  ]], shape=(5, 1), dtype=float32) \n",
      " ------------------- \n",
      " square_error:  tf.Tensor(\n",
      "[[0.02264582]\n",
      " [0.18705389]\n",
      " [0.5987689 ]\n",
      " [0.21543573]\n",
      " [0.33332932]], shape=(5, 1), dtype=float32) \n",
      " ------------------- \n",
      " delta_square / square_error:  tf.Tensor(\n",
      "[[38624.99    ]\n",
      " [   58.836983]\n",
      " [  336.87057 ]\n",
      " [ 6575.101   ]\n",
      " [ 4843.3765  ]], shape=(5, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"delta_square: \" , K.square(true - pred),\"\\n\",\"-------------------\",\"\\n\", \n",
    "      \"square_error: \" ,K.square(true_error),\"\\n\",\"-------------------\",\"\\n\", \n",
    "      \"delta_square / square_error: \" ,tf.divide(K.square(true - pred),K.square(true_error)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  tf.Tensor(10087.834, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"mean: \" , K.mean(tf.divide(K.square(true - pred),K.square(true_error))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrucción del Loss y del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_error): \n",
    "    def loss(y_true, y_pred):\n",
    "        #print(\"true: \",y_true, \"pred: \", y_pred, \"error: \", y_error)\n",
    "        delta_square = K.square(y_true - y_pred)\n",
    "        #print(\"delta_square: \",delta_square)\n",
    "        return  K.mean((1/2 * tf.divide(delta_square , K.square(y_error))) +\n",
    "                (1/2 * K.log(K.square(y_error))))       \n",
    "    return loss\n",
    "\n",
    "#INTENTAR DEFINIR EL MSE CON ESTA FUNCIÓN (ver si funciona hasta plot MAD)\n",
    "# El rendimiento de la red y la búsqueda de los pesos dependerá de la media???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ApogeeDR14GaiaDR2(dim_t , dim_n, dropout_iterations = 100): \n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    dim_t - number of time steps of spectrum \n",
    "    dim_n - number of features of spectrum\n",
    "    \"\"\"\n",
    "    \n",
    "    #SPECTRUM TO LUINOSITY\n",
    "    dim_1 = 1 # number of corrected magnitude for one example \n",
    "    units = 1 #number of final output for one example\n",
    "    \n",
    "    inputs_spectra = Input(shape=(dim_t, dim_n), name=\"pseudo-lum-input\") \n",
    "    inputs_mag = Input(shape=(dim_1,), name=\"K_mag\")\n",
    "    inputs_error_paralaje = Input(shape=(dim_1,), name=\"error_paralaje\")\n",
    "    print(\"inputs_mag: \",inputs_mag)\n",
    "    print(\"inputs_error_paralaje: \",inputs_error_paralaje)\n",
    "    \n",
    "    \n",
    "    #x_parallax_list = []\n",
    "    \n",
    "    #for i in range(droput_iterations):\n",
    "    x_parallax = Conv1D(filters=2, kernel_size=3, activation='relu')(inputs_spectra)\n",
    "    x_parallax = BatchNormalization()(x_parallax)\n",
    "    x_parallax = MaxPooling1D(pool_size=2)(x_parallax)\n",
    "    #x_parallax = Dropout(0.3)(x_parallax, training=True)\n",
    "\n",
    "    x_parallax = Conv1D(filters=4, kernel_size=3, activation='relu')(x_parallax)\n",
    "    x_parallax = BatchNormalization()(x_parallax)\n",
    "    x_parallax = MaxPooling1D(pool_size=2)(x_parallax)\n",
    "    #x_parallax = Dropout(0.3)(x_parallax, training=True)\n",
    "\n",
    "    x_parallax = Flatten()(x_parallax)\n",
    "    x_parallax = Dense(128, activation='relu')(x_parallax)\n",
    "    #x_parallax = Dropout(0.3)(x_parallax, training=True)\n",
    "        \n",
    "    x_parallax = Dense(64, activation='relu')(x_parallax) \n",
    "    #x_parallax = Dropout(0.3)(x_parallax, training=True)\n",
    "    x_parallax = Dense(32, activation='relu')(x_parallax)\n",
    "    #x_parallax = Dropout(0.3)(x_parallax, training=True)\n",
    "    x_parallax = Dense(units, activation='softplus', name=\"pseudo-lum\")(x_parallax) \n",
    "\n",
    "    #OFFSET CORRECTION : (optimization)\n",
    "    inputs_offset = Input(shape=(3,), name=\"offset-input\")\n",
    "    x_offset = Dense(64, activation='relu')(inputs_offset)\n",
    "    x_offset = Dense(32, activation='relu')(x_offset) \n",
    "    x_offset = Dense(units, activation='tanh', name=\"offset\")(x_offset) \n",
    "    \n",
    "    #Functions\n",
    "    outputs_parallax = Lambda(lambda function: tf.math.multiply(function[0], tf.math.pow(10., \n",
    "                              tf.math.multiply(-0.2, function[1]))),\n",
    "                              name='parallax')([x_parallax, inputs_mag])\n",
    "    \n",
    "    outputs_parallax_with_offset = Lambda(lambda function: tf.math.add(function[0], function[1]),\n",
    "                                          name=\"sum-parallax-offset\")([outputs_parallax, x_offset]) \n",
    "    \n",
    "    #Model setup\n",
    "    model =  Model(inputs = [inputs_spectra,inputs_mag, inputs_offset,inputs_error_paralaje],outputs = [outputs_parallax_with_offset])\n",
    "\n",
    "    model.compile(loss=custom_loss(y_error=inputs_error_paralaje), optimizer='adam', metrics=['mse']) \n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos\n",
    "path_local_data = '/home/anell/Desktop/Bovy/AnellExercises/Fits_files'\n",
    "#path_local_data = '/home/bapanes/Research-Now/local/astronn-local/apo-gaia/'\n",
    "\n",
    "with h5py.File(f'{path_local_data}/apogeedr14_gaiadr2_with_spectrum_probando_rendimiento_2.h5','r') as F:  \n",
    "    parallax = np.array(F['parallax'])\n",
    "    parallax_error = np.array(F['parallax_err'])\n",
    "    spectra = np.array(F['spectra'])\n",
    "    Kmag = np.array(F['corrected_magnitude_K'])\n",
    "    bp_rp = np.array(F['bp_rp'])\n",
    "    Gmag = np.array(F['phot_g_mean_mag'])\n",
    "    teff = np.array(F['NN_teff'])\n",
    "    apogee_id = np.array(F['APOGEE_ID'])\n",
    "    snr = np.array(F['SNR'])\n",
    "    fe_h = np.array(F['Fe/H'])\n",
    "    path_spectra = np.array(F['Path_spectra'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60986,), (60986, 7514), (60986,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallax.shape , spectra.shape , Kmag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establecemos las variables que entrarán a la red y corregimos sus dimensiones\n",
    "X = np.expand_dims(spectra, axis = 2)\n",
    "Y = np.expand_dims(parallax, axis = 1)\n",
    "K_mag = np.expand_dims(Kmag, axis = 1)\n",
    "Y_error = np.expand_dims(parallax_error, axis = 1)\n",
    "\n",
    "# Normalizamos Gmag , el color (G_bp - G_rp) y teff\n",
    "Gmag_std = np.std(Gmag)\n",
    "Gmag_mean = np.mean(Gmag)\n",
    "Gmag_norm = (Gmag - Gmag_mean) / Gmag_std\n",
    "\n",
    "bp_rp_std = np.std(bp_rp)\n",
    "bp_rp_mean = np.mean(bp_rp)\n",
    "bp_rp_norm = (bp_rp - bp_rp_mean) / bp_rp_std\n",
    "\n",
    "teff_std = np.std(teff)\n",
    "teff_mean = np.mean(teff)\n",
    "teff_norm = (teff - teff_mean) / teff_std\n",
    "\n",
    "G_mag = np.expand_dims(Gmag_norm, axis=1)\n",
    "Bp_Rp = np.expand_dims(bp_rp_norm, axis=1)\n",
    "Teff = np.expand_dims(teff_norm, axis=1)\n",
    "\n",
    "X_offset = np.concatenate((G_mag, Bp_Rp , Teff), axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60986, 7514, 1) (60986, 1) (60986, 1) (60986, 3) (60986, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape , Y.shape, K_mag.shape, X_offset.shape,Y_error.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNR cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_snr_idx = []\n",
    "low_snr_idx = []\n",
    "\n",
    "for i in range(len(snr)):\n",
    "    if snr[i] >= 200:           \n",
    "        high_snr_idx.append(i)\n",
    "    else:\n",
    "        low_snr_idx.append(i)\n",
    "\n",
    "random.seed(50)\n",
    "random.shuffle(high_snr_idx)\n",
    "random.seed(200)\n",
    "random.shuffle(low_snr_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR>200: 27721, else: 33265\n"
     ]
    }
   ],
   "source": [
    "print(\"SNR>200: %d, else: %d\"%(len(high_snr_idx), len(low_snr_idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diseño Experimental:\n",
    "\n",
    "Consideraré train (0.8) y valid (0.2)\n",
    "\n",
    "train_val_1 $\\rightarrow$  177 (train) + 44 (valid) = 221\n",
    "\n",
    "train_val_2 $\\rightarrow$  400 (train) + 100 (valid) = 500\n",
    "\n",
    "train_val_3 $\\rightarrow$  800 (train) + 200 (valid) = 1000\n",
    "\n",
    "train_val_4 $\\rightarrow$  2400 (train) + 600 (valid) = 3000\n",
    "\n",
    "train_val_5 $\\rightarrow$  6400 (train) + 1600 (valid) = 8000\n",
    "\n",
    "train_val_6 $\\rightarrow$  12000 (train) + 3000 (valid) = 15000\n",
    "\n",
    "train_val_7 $\\rightarrow$  22177 (train) + 5594 (valid) = 27771\n",
    "\n",
    "test $\\rightarrow$ 33265"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_high_snr = X[high_snr_idx]\n",
    "Y_high_snr = Y[high_snr_idx]\n",
    "K_mag_high_snr = K_mag[high_snr_idx]\n",
    "X_offset_high_snr = X_offset[high_snr_idx]\n",
    "Y_error_high_snr = Y_error[high_snr_idx]\n",
    "\n",
    "X_low_snr = X[low_snr_idx]\n",
    "Y_low_snr = Y[low_snr_idx]\n",
    "K_mag_low_snr = K_mag[low_snr_idx]\n",
    "X_offset_low_snr = X_offset[low_snr_idx]\n",
    "Y_error_low_snr = Y_error[low_snr_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val_1 = X_high_snr[:221]\n",
    "Y_train_val_1 = Y_high_snr[:221]\n",
    "K_mag_train_val_1 = K_mag_high_snr[:221]\n",
    "X_offset_train_val_1 = X_offset_high_snr[:221]\n",
    "Y_error_train_val_1 = Y_error_high_snr[:221]\n",
    "\n",
    "X_train_val_2 = X_high_snr[221:721]\n",
    "Y_train_val_2 = Y_high_snr[221:721]\n",
    "K_mag_train_val_2 = K_mag_high_snr[221:721]\n",
    "X_offset_train_val_2 = X_offset_high_snr[221:721]\n",
    "Y_error_train_val_2 = Y_error_high_snr[221:721]\n",
    "\n",
    "X_train_val_3 = X_high_snr[721:1721]\n",
    "Y_train_val_3 = Y_high_snr[721:1721]\n",
    "K_mag_train_val_3 = K_mag_high_snr[721:1721]\n",
    "X_offset_train_val_3 = X_offset_high_snr[721:1721]\n",
    "Y_error_train_val_3 = Y_error_high_snr[721:1721]\n",
    "\n",
    "X_train_val_4 = X_high_snr[1721:4721]\n",
    "Y_train_val_4 = Y_high_snr[1721:4721]\n",
    "K_mag_train_val_4 = K_mag_high_snr[1721:4721]\n",
    "X_offset_train_val_4 = X_offset_high_snr[1721:4721]\n",
    "Y_error_train_val_4 = Y_error_high_snr[1721:4721]\n",
    "\n",
    "X_train_val_5 = X_high_snr[4721:12721]\n",
    "Y_train_val_5 = Y_high_snr[4721:12721]\n",
    "K_mag_train_val_5 = K_mag_high_snr[4721:12721]\n",
    "X_offset_train_val_5 = X_offset_high_snr[4721:12721]\n",
    "Y_error_train_val_5 = Y_error_high_snr[4721:12721]\n",
    "\n",
    "X_train_val_6 = X_high_snr[12721:]\n",
    "Y_train_val_6 = Y_high_snr[12721:]\n",
    "K_mag_train_val_6 = K_mag_high_snr[12721:]\n",
    "X_offset_train_val_6 = X_offset_high_snr[12721:]\n",
    "Y_error_train_val_6 = Y_error_high_snr[12721:]\n",
    "\n",
    "X_train_val_7 = X_high_snr\n",
    "Y_train_val_7 = Y_high_snr\n",
    "K_mag_train_val_7 = K_mag_high_snr\n",
    "X_offset_train_val_7 = X_offset_high_snr\n",
    "Y_error_train_val_7 = Y_error_high_snr\n",
    "\n",
    "X_test = X_low_snr\n",
    "Y_test = Y_low_snr\n",
    "K_mag_test = K_mag_low_snr\n",
    "X_offset_test = X_offset_low_snr\n",
    "Y_error_test = Y_error_low_snr\n",
    "snr_test = snr[low_snr_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(221, 1) (500, 1) (1000, 1) (3000, 1) (8000, 1) (15000, 1) (27721, 1) (33265, 1)\n"
     ]
    }
   ],
   "source": [
    "print(Y_train_val_1.shape , Y_train_val_2.shape , Y_train_val_3.shape, Y_train_val_4.shape , \n",
    "      Y_train_val_5.shape,Y_train_val_6.shape ,Y_train_val_7.shape , Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_mag:  Tensor(\"K_mag_3:0\", shape=(None, 1), dtype=float32)\n",
      "inputs_error_paralaje:  Tensor(\"error_paralaje_3:0\", shape=(None, 1), dtype=float32)\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "pseudo-lum-input (InputLayer)   (None, 7514, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 7512, 2)      8           pseudo-lum-input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 7512, 2)      8           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 3756, 2)      0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 3754, 4)      28          max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 3754, 4)      16          conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1877, 4)      0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 7508)         0           max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 128)          961152      flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 64)           8256        dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "offset-input (InputLayer)       (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 32)           2080        dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 64)           256         offset-input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "pseudo-lum (Dense)              (None, 1)            33          dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "K_mag (InputLayer)              (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 32)           2080        dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "parallax (Lambda)               (None, 1)            0           pseudo-lum[0][0]                 \n",
      "                                                                 K_mag[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "offset (Dense)                  (None, 1)            33          dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sum-parallax-offset (Lambda)    (None, 1)            0           parallax[0][0]                   \n",
      "                                                                 offset[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 973,950\n",
      "Trainable params: 973,938\n",
      "Non-trainable params: 12\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_timesteps, n_features = X_train_val_7.shape[1], X_train_val_7.shape[2]\n",
    "\n",
    "Global_model = ApogeeDR14GaiaDR2(n_timesteps , n_features)\n",
    "\n",
    "Global_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22176 samples, validate on 5545 samples\n",
      "Epoch 1/200\n",
      "22176/22176 [==============================] - 21s 967us/step - loss: 1633.3677 - mse: 4.6405 - val_loss: 9235.9720 - val_mse: 19.8232\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 9235.97199, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D7_ReplicaBovy_Custom_Loss_shuffle3.h5\n",
      "Epoch 2/200\n",
      "22176/22176 [==============================] - 18s 791us/step - loss: 337.1963 - mse: 1.1600 - val_loss: 3194.6610 - val_mse: 8.4563\n",
      "\n",
      "Epoch 00002: val_loss improved from 9235.97199 to 3194.66095, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D7_ReplicaBovy_Custom_Loss_shuffle3.h5\n",
      "Epoch 3/200\n",
      "22176/22176 [==============================] - 18s 828us/step - loss: 218.3855 - mse: 0.8121 - val_loss: 1799.5818 - val_mse: 3.8868\n",
      "\n",
      "Epoch 00003: val_loss improved from 3194.66095 to 1799.58179, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D7_ReplicaBovy_Custom_Loss_shuffle3.h5\n",
      "Epoch 4/200\n",
      "22176/22176 [==============================] - 19s 845us/step - loss: 170.7735 - mse: 0.6123 - val_loss: 2087.0445 - val_mse: 4.1036\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1799.58179\n",
      "Epoch 5/200\n",
      "22176/22176 [==============================] - 19s 852us/step - loss: 145.8653 - mse: 0.5274 - val_loss: 3087.3017 - val_mse: 5.8229\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1799.58179\n",
      "Epoch 6/200\n",
      "22176/22176 [==============================] - 19s 847us/step - loss: 113.5268 - mse: 0.4210 - val_loss: 2677.1785 - val_mse: 5.0195\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1799.58179\n",
      "Epoch 7/200\n",
      "22176/22176 [==============================] - 19s 841us/step - loss: 94.3555 - mse: 0.3614 - val_loss: 1942.9352 - val_mse: 3.6970\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1799.58179\n",
      "Epoch 8/200\n",
      "22176/22176 [==============================] - 19s 841us/step - loss: 89.9558 - mse: 0.3458 - val_loss: 2042.7061 - val_mse: 3.8683\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1799.58179\n",
      "Epoch 9/200\n",
      "22176/22176 [==============================] - 19s 842us/step - loss: 71.1473 - mse: 0.2769 - val_loss: 2199.6521 - val_mse: 4.1066\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1799.58179\n",
      "Epoch 10/200\n",
      "22176/22176 [==============================] - 19s 845us/step - loss: 68.8766 - mse: 0.2699 - val_loss: 2187.5092 - val_mse: 4.0620\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1799.58179\n",
      "Epoch 11/200\n",
      "22176/22176 [==============================] - 19s 839us/step - loss: 64.2709 - mse: 0.2594 - val_loss: 2328.2459 - val_mse: 4.3030\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1799.58179\n",
      "Epoch 12/200\n",
      "22176/22176 [==============================] - 19s 840us/step - loss: 58.2096 - mse: 0.2360 - val_loss: 2106.9836 - val_mse: 3.9004\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1799.58179\n",
      "Epoch 13/200\n",
      "22176/22176 [==============================] - 19s 840us/step - loss: 62.2125 - mse: 0.2374 - val_loss: 2002.1920 - val_mse: 3.7298\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1799.58179\n",
      "Epoch 14/200\n",
      "22176/22176 [==============================] - 19s 842us/step - loss: 51.9962 - mse: 0.2049 - val_loss: 2173.2778 - val_mse: 4.0227\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1799.58179\n",
      "Epoch 15/200\n",
      "22176/22176 [==============================] - 19s 843us/step - loss: 48.8099 - mse: 0.1989 - val_loss: 1873.8989 - val_mse: 3.4690\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1799.58179\n",
      "Epoch 16/200\n",
      "22176/22176 [==============================] - 19s 844us/step - loss: 48.2634 - mse: 0.1920 - val_loss: 2193.4018 - val_mse: 4.0403\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1799.58179\n",
      "Epoch 17/200\n",
      "22176/22176 [==============================] - 19s 847us/step - loss: 47.1516 - mse: 0.1912 - val_loss: 2098.5410 - val_mse: 3.8602\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1799.58179\n",
      "Epoch 18/200\n",
      "22176/22176 [==============================] - 19s 843us/step - loss: 45.4071 - mse: 0.1884 - val_loss: 2219.0942 - val_mse: 4.0856\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1799.58179\n",
      "Epoch 19/200\n",
      "22176/22176 [==============================] - 19s 843us/step - loss: 42.2717 - mse: 0.1733 - val_loss: 2093.2578 - val_mse: 3.8601\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1799.58179\n",
      "Epoch 20/200\n",
      "22176/22176 [==============================] - 19s 843us/step - loss: 41.3868 - mse: 0.1734 - val_loss: 2023.1185 - val_mse: 3.7359\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1799.58179\n",
      "Epoch 21/200\n",
      "22176/22176 [==============================] - 19s 843us/step - loss: 41.3389 - mse: 0.1730 - val_loss: 2057.0169 - val_mse: 3.7858\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1799.58179\n",
      "Epoch 22/200\n",
      "22176/22176 [==============================] - 19s 864us/step - loss: 41.2341 - mse: 0.1706 - val_loss: 2132.0501 - val_mse: 3.9186\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1799.58179\n",
      "Epoch 23/200\n",
      "22176/22176 [==============================] - 18s 833us/step - loss: 40.0793 - mse: 0.1675 - val_loss: 2227.0505 - val_mse: 4.1000\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1799.58179\n",
      "Epoch 24/200\n",
      "22176/22176 [==============================] - 18s 832us/step - loss: 38.8181 - mse: 0.1631 - val_loss: 2057.1561 - val_mse: 3.7883\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1799.58179\n",
      "Epoch 25/200\n",
      "22176/22176 [==============================] - 18s 831us/step - loss: 37.6091 - mse: 0.1601 - val_loss: 2103.7371 - val_mse: 3.8743\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1799.58179\n",
      "Epoch 26/200\n",
      "22176/22176 [==============================] - 18s 831us/step - loss: 37.0480 - mse: 0.1582 - val_loss: 2031.9225 - val_mse: 3.7404\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1799.58179\n",
      "Epoch 27/200\n",
      "22176/22176 [==============================] - 18s 831us/step - loss: 37.1967 - mse: 0.1581 - val_loss: 1967.4195 - val_mse: 3.6234\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1799.58179\n",
      "Epoch 28/200\n",
      "22176/22176 [==============================] - 18s 831us/step - loss: 36.9787 - mse: 0.1570 - val_loss: 2050.9461 - val_mse: 3.7770\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1799.58179\n",
      "Epoch 29/200\n",
      "22176/22176 [==============================] - 18s 833us/step - loss: 35.8786 - mse: 0.1542 - val_loss: 2097.7599 - val_mse: 3.8582\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1799.58179\n",
      "Epoch 30/200\n",
      "22176/22176 [==============================] - 18s 833us/step - loss: 35.6751 - mse: 0.1533 - val_loss: 2097.3771 - val_mse: 3.8592\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1799.58179\n",
      "Epoch 31/200\n",
      "22176/22176 [==============================] - 18s 832us/step - loss: 35.3698 - mse: 0.1531 - val_loss: 2015.4273 - val_mse: 3.7085\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1799.58179\n",
      "Epoch 32/200\n",
      "22176/22176 [==============================] - 18s 831us/step - loss: 35.3229 - mse: 0.1513 - val_loss: 2089.6740 - val_mse: 3.8446\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1799.58179\n",
      "Epoch 33/200\n",
      "22176/22176 [==============================] - 18s 832us/step - loss: 35.3268 - mse: 0.1530 - val_loss: 2097.0897 - val_mse: 3.8581\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1799.58179\n",
      "Epoch 34/200\n",
      "22176/22176 [==============================] - 18s 832us/step - loss: 34.6566 - mse: 0.1503 - val_loss: 1999.6321 - val_mse: 3.6809\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1799.58179\n",
      "Epoch 35/200\n",
      "22176/22176 [==============================] - 18s 833us/step - loss: 34.7402 - mse: 0.1491 - val_loss: 2086.4946 - val_mse: 3.8377\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1799.58179\n",
      "Epoch 36/200\n",
      "22176/22176 [==============================] - 19s 836us/step - loss: 34.4153 - mse: 0.1490 - val_loss: 2021.0776 - val_mse: 3.7187\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1799.58179\n",
      "Epoch 37/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22176/22176 [==============================] - 18s 800us/step - loss: 34.4504 - mse: 0.1489 - val_loss: 2062.1386 - val_mse: 3.7927\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1799.58179\n",
      "Epoch 38/200\n",
      "22176/22176 [==============================] - 18s 798us/step - loss: 34.2580 - mse: 0.1483 - val_loss: 2020.3497 - val_mse: 3.7178\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1799.58179\n",
      "Epoch 39/200\n",
      "22176/22176 [==============================] - 18s 798us/step - loss: 33.9779 - mse: 0.1470 - val_loss: 2048.0708 - val_mse: 3.7680\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1799.58179\n",
      "Epoch 40/200\n",
      "22176/22176 [==============================] - 18s 799us/step - loss: 33.9634 - mse: 0.1475 - val_loss: 2016.5974 - val_mse: 3.7105\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1799.58179\n",
      "Epoch 41/200\n",
      "22176/22176 [==============================] - 18s 799us/step - loss: 33.8252 - mse: 0.1462 - val_loss: 2026.6328 - val_mse: 3.7292\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1799.58179\n",
      "Epoch 42/200\n",
      "22176/22176 [==============================] - 18s 803us/step - loss: 34.0360 - mse: 0.1474 - val_loss: 2071.8353 - val_mse: 3.8107\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1799.58179\n",
      "Epoch 43/200\n",
      "22176/22176 [==============================] - 21s 928us/step - loss: 33.9627 - mse: 0.1476 - val_loss: 2058.0183 - val_mse: 3.7857\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1799.58179\n",
      "Epoch 44/200\n",
      "22176/22176 [==============================] - 18s 821us/step - loss: 33.7216 - mse: 0.1463 - val_loss: 2057.5056 - val_mse: 3.7845\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1799.58179\n",
      "Epoch 45/200\n",
      "22176/22176 [==============================] - 19s 858us/step - loss: 33.7492 - mse: 0.1467 - val_loss: 2052.5826 - val_mse: 3.7756\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1799.58179\n",
      "Epoch 46/200\n",
      "22176/22176 [==============================] - 18s 828us/step - loss: 33.6468 - mse: 0.1465 - val_loss: 2020.2607 - val_mse: 3.7169\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1799.58179\n",
      "Epoch 47/200\n",
      "22176/22176 [==============================] - 18s 800us/step - loss: 33.7143 - mse: 0.1464 - val_loss: 2038.8806 - val_mse: 3.7508\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1799.58179\n",
      "Epoch 48/200\n",
      "22176/22176 [==============================] - 18s 800us/step - loss: 33.6364 - mse: 0.1457 - val_loss: 2042.9159 - val_mse: 3.7583\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1799.58179\n",
      "Epoch 49/200\n",
      "22176/22176 [==============================] - 18s 795us/step - loss: 33.5508 - mse: 0.1461 - val_loss: 2035.7291 - val_mse: 3.7449\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1799.58179\n",
      "Epoch 50/200\n",
      "22176/22176 [==============================] - 18s 794us/step - loss: 33.5201 - mse: 0.1458 - val_loss: 2032.6686 - val_mse: 3.7395\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1799.58179\n",
      "Epoch 51/200\n",
      "22176/22176 [==============================] - 18s 794us/step - loss: 33.5215 - mse: 0.1456 - val_loss: 2037.9453 - val_mse: 3.7493\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1799.58179\n",
      "Epoch 52/200\n",
      "22176/22176 [==============================] - 18s 794us/step - loss: 33.4969 - mse: 0.1456 - val_loss: 2046.8538 - val_mse: 3.7651\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1799.58179\n",
      "Epoch 53/200\n",
      "22176/22176 [==============================] - 18s 793us/step - loss: 33.4741 - mse: 0.1455 - val_loss: 2049.5754 - val_mse: 3.7700\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1799.58179\n",
      "Epoch 00053: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f6f0005ef90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Global_model.compile(optimizer='adam', loss= None, metrics=['mse'])\n",
    "\n",
    "path_local = \"/home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss\"\n",
    "\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=50, verbose=1, min_delta=1e-7)\n",
    "checkpoint = ModelCheckpoint(f'{path_local}/Modelo_1_D7_ReplicaBovy_Custom_Loss_shuffle3.h5', monitor='val_loss', \n",
    "                             verbose=1, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, verbose=1, patience=5, min_lr=0.000000001)\n",
    "\n",
    "callbacks=[reduce_lr, checkpoint, earlystopper]\n",
    "\n",
    "Global_model.fit([X_train_val_7, K_mag_train_val_7, X_offset_train_val_7,Y_error_train_val_7], Y_train_val_7,epochs=200, \n",
    "                 batch_size=128, verbose=1, shuffle=\"batch\" ,callbacks=callbacks,validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### simple evaluations on test sample (SNR < 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33265/33265 [==============================] - 11s 322us/step\n",
      "144268.4252838208 1029.5679931640625\n"
     ]
    }
   ],
   "source": [
    "J_test , mse_test = Global_model.evaluate([X_test, K_mag_test , X_offset_test, Y_error_test], Y_test)\n",
    "print(J_test,mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 3s 330us/step\n",
      "94.5589104604721 0.34270167350769043\n"
     ]
    }
   ],
   "source": [
    "J_train , mse_train = Global_model.evaluate([X_train_val_5, K_mag_train_val_5 , X_offset_train_val_5, Y_error_train_val_5], Y_train_val_5)\n",
    "print(J_train,mse_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle 2:\n",
    "\n",
    "(No se guardan por la métrica MSE, si no que por mi métrica)\n",
    "\n",
    "Datos de entrenamiento || Epocas || MSE train || MSE valid || MSE test || MSE train-val\n",
    "\n",
    "    Train_valid_3         200/200     0.23         3.42         1286        0.88       \n",
    "    Train_valid_4         146/200     0.103        1.31         3086        0.35   \n",
    "    Train_valid_5         126/200     0.125        0.30         1238        0.14   \n",
    "    Train_valid_6         123/200     0.049        0.32         1029        0.34\n",
    "    Train_valid_7         88/200      0.038        2.36         1143        0.53   \n",
    "\n",
    "\n",
    "Dado el comportamiento extraño del MSE del train set, veré la distribución de errores (parallax_error) de cada train_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(Y_error_train_val_7 , bins = 100 , alpha=0.3,color=\"g\")\n",
    "plt.hist(Y_error_train_val_6 , bins = 100 , alpha=0.5,color=\"m\")\n",
    "plt.hist(Y_error_train_val_5 , bins = 100 , alpha=0.7,color=\"c\")\n",
    "plt.hist(Y_error_train_val_4 , bins = 80 , alpha=0.9, color=\"r\")\n",
    "plt.hist(Y_error_train_val_3 , bins = 50 , alpha=1, color=\"b\")\n",
    "#plt.xlim(-5,30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9342359767891683\n",
      "0.9556714471968709\n",
      "0.9960079840319361\n",
      "0.9595035924232528\n",
      "0.971341203242782\n"
     ]
    }
   ],
   "source": [
    "def razon_errores(valores , limite):\n",
    "    menores = []\n",
    "    mayores = []\n",
    "    for i in range(len(valores)):\n",
    "        if valores[i] <= limite:\n",
    "            menores.append(i)\n",
    "        else:\n",
    "            mayores.append(i)\n",
    "    print(len(menores) / (len(mayores) ))\n",
    "\n",
    "razon_train_val_3 = razon_errores(Y_error_train_val_3 , 0.03)\n",
    "razon_train_val_4 = razon_errores(Y_error_train_val_4 , 0.03)\n",
    "razon_train_val_5 = razon_errores(Y_error_train_val_5 , 0.03)\n",
    "razon_train_val_6 = razon_errores(Y_error_train_val_6 , 0.03)\n",
    "razon_train_val_7 = razon_errores(Y_error_train_val_7 , 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " promedio error train_val_3:  3.331753688968634 \n",
      " promedio error train_val_4:  3.3450137762863723 \n",
      " promedio error train_val_5:  3.339581431179543 \n",
      " promedio error train_val_6:  3.348711253811788 \n",
      " promedio error train_val_7:  3.344231888818594\n",
      "--------------------------------------------------------------------\n",
      " mediana error train_val_3:  3.0701351243777815 \n",
      " mediana error train_val_4:  3.0344057605714823 \n",
      " mediana error train_val_5:  3.002729816852311 \n",
      " mediana error train_val_6:  3.039096053210532 \n",
      " mediana error train_val_7:  3.0272296183079717\n"
     ]
    }
   ],
   "source": [
    "print(\" promedio error train_val_3: \" ,  np.mean(Y_error_train_val_3)*100, \"\\n\",\n",
    "     \"promedio error train_val_4: \" ,  np.mean(Y_error_train_val_4)*100, \"\\n\",\n",
    "     \"promedio error train_val_5: \" ,  np.mean(Y_error_train_val_5)*100, \"\\n\",\n",
    "     \"promedio error train_val_6: \" ,  np.mean(Y_error_train_val_6)*100, \"\\n\",\n",
    "     \"promedio error train_val_7: \" ,  np.mean(Y_error_train_val_7)*100)\n",
    "print(\"--------------------------------------------------------------------\")\n",
    "print(\" mediana error train_val_3: \" ,  np.median(Y_error_train_val_3)*100, \"\\n\",\n",
    "     \"mediana error train_val_4: \" ,  np.median(Y_error_train_val_4)*100, \"\\n\",\n",
    "     \"mediana error train_val_5: \" ,  np.median(Y_error_train_val_5)*100, \"\\n\",\n",
    "     \"mediana error train_val_6: \" ,  np.median(Y_error_train_val_6)*100, \"\\n\",\n",
    "     \"mediana error train_val_7: \" ,  np.median(Y_error_train_val_7)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAD_pred(Y,Y_hat):  \n",
    "    R_percent = (Y - Y_hat) / Y *100\n",
    "    s_mad = 1.4826 * np.median(np.abs(R_percent - np.median(R_percent)))\n",
    "    return s_mad\n",
    "\n",
    "def MAD_residual(R_percent):  \n",
    "    s_mad = 1.4826 * np.median(np.abs(R_percent - np.median(R_percent)))\n",
    "    return s_mad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mad error train_val_3:  1.152596100863585 \n",
      " mad error train_val_4:  1.1096577875449283 \n",
      " mad error train_val_5:  1.108000843576499 \n",
      " mad error train_val_6:  1.140506975434073 \n",
      " mad error train_val_7:  1.1288166654050544\n"
     ]
    }
   ],
   "source": [
    "mad_error_train_val_3 = MAD_residual(Y_error_train_val_3*100)\n",
    "mad_error_train_val_4 = MAD_residual(Y_error_train_val_4*100)\n",
    "mad_error_train_val_5 = MAD_residual(Y_error_train_val_5*100)\n",
    "mad_error_train_val_6 = MAD_residual(Y_error_train_val_6*100)\n",
    "mad_error_train_val_7 = MAD_residual(Y_error_train_val_7*100)\n",
    "\n",
    "print(\"mad error train_val_3: \", mad_error_train_val_3,\"\\n\",\n",
    "     \"mad error train_val_4: \", mad_error_train_val_4,\"\\n\",\n",
    "     \"mad error train_val_5: \", mad_error_train_val_5,\"\\n\",\n",
    "     \"mad error train_val_6: \", mad_error_train_val_6,\"\\n\",\n",
    "     \"mad error train_val_7: \", mad_error_train_val_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = Global_model.predict([X_test, K_mag_test , X_offset_test, Y_error_test])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
