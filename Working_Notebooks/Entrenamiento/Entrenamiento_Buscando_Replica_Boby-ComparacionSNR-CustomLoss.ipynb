{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.layers import Conv1D , Dropout , Flatten , MaxPooling1D, Dense, Input, BatchNormalization , Dropout\n",
    "from keras.layers.core import Lambda\n",
    "from keras.models import Model , load_model , model_from_json\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import h5py\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_error): \n",
    "    def loss(y_true, y_pred):\n",
    "        delta = K.square(y_true - y_pred)\n",
    "        s = K.log(K.square(y_error))   \n",
    "        return tf.divide((1/2 * delta) , K.square(y_error) ) + (1/2 * s)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ApogeeDR14GaiaDR2(dim_t , dim_n, dropout_iterations = 100): \n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    dim_t - number of time steps of spectrum \n",
    "    dim_n - number of features of spectrum\n",
    "    \"\"\"\n",
    "    \n",
    "    #SPECTRUM TO LUINOSITY\n",
    "    dim_1 = 1 # number of corrected magnitude for one example \n",
    "    units = 1 #number of final output for one example\n",
    "    \n",
    "    inputs_spectra = Input(shape=(dim_t, dim_n), name=\"pseudo-lum-input\") \n",
    "    inputs_mag = Input(shape=(dim_1,), name=\"K_mag\")\n",
    "    inputs_error_paralaje = Input(shape=(dim_1,), name=\"error_paralaje\")\n",
    "    \n",
    "    #x_parallax_list = []\n",
    "    \n",
    "    #for i in range(droput_iterations):\n",
    "    x_parallax = Conv1D(filters=2, kernel_size=3, activation='relu')(inputs_spectra)\n",
    "    x_parallax = BatchNormalization()(x_parallax)\n",
    "    x_parallax = MaxPooling1D(pool_size=2)(x_parallax)\n",
    "    #x_parallax = Dropout(0.3)(x_parallax, training=True)\n",
    "\n",
    "    x_parallax = Conv1D(filters=4, kernel_size=3, activation='relu')(x_parallax)\n",
    "    x_parallax = BatchNormalization()(x_parallax)\n",
    "    x_parallax = MaxPooling1D(pool_size=2)(x_parallax)\n",
    "    #x_parallax = Dropout(0.3)(x_parallax, training=True)\n",
    "\n",
    "    x_parallax = Flatten()(x_parallax)\n",
    "    x_parallax = Dense(128, activation='relu')(x_parallax)\n",
    "    #x_parallax = Dropout(0.3)(x_parallax, training=True)\n",
    "        \n",
    "    x_parallax = Dense(64, activation='relu')(x_parallax) \n",
    "    #x_parallax = Dropout(0.3)(x_parallax, training=True)\n",
    "    x_parallax = Dense(32, activation='relu')(x_parallax)\n",
    "    #x_parallax = Dropout(0.3)(x_parallax, training=True)\n",
    "    x_parallax = Dense(units, activation='softplus', name=\"pseudo-lum\")(x_parallax) \n",
    "\n",
    "    #OFFSET CORRECTION : (optimization)\n",
    "    inputs_offset = Input(shape=(3,), name=\"offset-input\")\n",
    "    x_offset = Dense(64, activation='relu')(inputs_offset)\n",
    "    x_offset = Dense(32, activation='relu')(x_offset) \n",
    "    x_offset = Dense(units, activation='tanh', name=\"offset\")(x_offset) \n",
    "    \n",
    "    #Functions\n",
    "    outputs_parallax = Lambda(lambda function: tf.math.multiply(function[0], tf.math.pow(10., \n",
    "                              tf.math.multiply(-0.2, function[1]))),\n",
    "                              name='parallax')([x_parallax, inputs_mag])\n",
    "    \n",
    "    outputs_parallax_with_offset = Lambda(lambda function: tf.math.add(function[0], function[1]),\n",
    "                                          name=\"sum-parallax-offset\")([outputs_parallax, x_offset]) \n",
    "    \n",
    "    #Model setup\n",
    "    model =  Model(inputs = [inputs_spectra,inputs_mag, inputs_offset,inputs_error_paralaje],outputs = [outputs_parallax_with_offset])\n",
    "    \n",
    "    model.compile(loss=[custom_loss(y_error=inputs_error_paralaje)], optimizer='adam', metrics=['mse']) \n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos\n",
    "path_local_data = '/home/anell/Desktop/Bovy/AnellExercises/Fits_files'\n",
    "#path_local_data = '/home/bapanes/Research-Now/local/astronn-local/apo-gaia/'\n",
    "\n",
    "with h5py.File(f'{path_local_data}/apogeedr14_gaiadr2_with_spectrum_probando_rendimiento_2.h5','r') as F:  \n",
    "    parallax = np.array(F['parallax'])\n",
    "    parallax_error = np.array(F['parallax_err'])\n",
    "    spectra = np.array(F['spectra'])\n",
    "    Kmag = np.array(F['corrected_magnitude_K'])\n",
    "    bp_rp = np.array(F['bp_rp'])\n",
    "    Gmag = np.array(F['phot_g_mean_mag'])\n",
    "    teff = np.array(F['NN_teff'])\n",
    "    apogee_id = np.array(F['APOGEE_ID'])\n",
    "    snr = np.array(F['SNR'])\n",
    "    fe_h = np.array(F['Fe/H'])\n",
    "    path_spectra = np.array(F['Path_spectra'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60986,), (60986, 7514), (60986,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallax.shape , spectra.shape , Kmag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establecemos las variables que entrarán a la red y corregimos sus dimensiones\n",
    "X = np.expand_dims(spectra, axis = 2)\n",
    "Y = np.expand_dims(parallax, axis = 1)\n",
    "K_mag = np.expand_dims(Kmag, axis = 1)\n",
    "Y_error = np.expand_dims(parallax_error, axis = 1)\n",
    "\n",
    "# Normalizamos Gmag , el color (G_bp - G_rp) y teff\n",
    "Gmag_std = np.std(Gmag)\n",
    "Gmag_mean = np.mean(Gmag)\n",
    "Gmag_norm = (Gmag - Gmag_mean) / Gmag_std\n",
    "\n",
    "bp_rp_std = np.std(bp_rp)\n",
    "bp_rp_mean = np.mean(bp_rp)\n",
    "bp_rp_norm = (bp_rp - bp_rp_mean) / bp_rp_std\n",
    "\n",
    "teff_std = np.std(teff)\n",
    "teff_mean = np.mean(teff)\n",
    "teff_norm = (teff - teff_mean) / teff_std\n",
    "\n",
    "G_mag = np.expand_dims(Gmag_norm, axis=1)\n",
    "Bp_Rp = np.expand_dims(bp_rp_norm, axis=1)\n",
    "Teff = np.expand_dims(teff_norm, axis=1)\n",
    "\n",
    "X_offset = np.concatenate((G_mag, Bp_Rp , Teff), axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60986, 7514, 1) (60986, 1) (60986, 1) (60986, 3) (60986, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape , Y.shape, K_mag.shape, X_offset.shape,Y_error.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNR cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_snr_idx = []\n",
    "low_snr_idx = []\n",
    "\n",
    "for i in range(len(snr)):\n",
    "    if snr[i] >= 200:           \n",
    "        high_snr_idx.append(i)\n",
    "    else:\n",
    "        low_snr_idx.append(i)\n",
    "\n",
    "random.seed(10)\n",
    "random.shuffle(high_snr_idx)\n",
    "random.seed(60)\n",
    "random.shuffle(low_snr_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estos valores pueden ser comparados con Bovy train-test separation, \n",
    "\n",
    "Apart from taht we should be more clear about data preparation and evaluation\n",
    "\n",
    "get consistent and repeatable accuracy using train and validation approach <br>\n",
    "undestand the process of validation (draft about probability_distributions) <br> \n",
    "check the precision on the test data using baseline separation <br>\n",
    "compute learning curve, following Fig 1. of Nguyen et. al.<br>\n",
    "try with percentual mse, such that low and high parallax values weight the same <br>\n",
    "\n",
    "initially we can consider SNR>200 for train-valid and SNR<200 for test <br>\n",
    "understand why it is possible to make this separation and hope for consistent results <br>\n",
    "\n",
    "compare to Bovy using this data separation, using global precision and learning curve<br>\n",
    "\n",
    "How well are we doing with respect to Bovy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image(\"/home/bapanes/Research-Now/TesisAnell/Figures/learning_curve_genoma.png\")\n",
    "#Image(\"learning_curve_genoma.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR>200: 27721, else: 33265\n"
     ]
    }
   ],
   "source": [
    "print(\"SNR>200: %d, else: %d\"%(len(high_snr_idx), len(low_snr_idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diseño Experimental:\n",
    "\n",
    "Consideraré train (0.8) y valid (0.2)\n",
    "\n",
    "train_val_1 $\\rightarrow$  177 (train) + 44 (valid) = 221\n",
    "\n",
    "train_val_2 $\\rightarrow$  400 (train) + 100 (valid) = 500\n",
    "\n",
    "train_val_3 $\\rightarrow$  800 (train) + 200 (valid) = 1000\n",
    "\n",
    "train_val_4 $\\rightarrow$  2400 (train) + 600 (valid) = 3000\n",
    "\n",
    "train_val_5 $\\rightarrow$  6400 (train) + 1600 (valid) = 8000\n",
    "\n",
    "train_val_6 $\\rightarrow$  12000 (train) + 3000 (valid) = 15000\n",
    "\n",
    "train_val_7 $\\rightarrow$  22177 (train) + 5594 (valid) = 27771\n",
    "\n",
    "test $\\rightarrow$ 33265"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_high_snr = X[high_snr_idx]\n",
    "Y_high_snr = Y[high_snr_idx]\n",
    "K_mag_high_snr = K_mag[high_snr_idx]\n",
    "X_offset_high_snr = X_offset[high_snr_idx]\n",
    "Y_error_high_snr = Y_error[high_snr_idx]\n",
    "\n",
    "X_low_snr = X[low_snr_idx]\n",
    "Y_low_snr = Y[low_snr_idx]\n",
    "K_mag_low_snr = K_mag[low_snr_idx]\n",
    "X_offset_low_snr = X_offset[low_snr_idx]\n",
    "Y_error_low_snr = Y_error[low_snr_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val_1 = X_high_snr[:221]\n",
    "Y_train_val_1 = Y_high_snr[:221]\n",
    "K_mag_train_val_1 = K_mag_high_snr[:221]\n",
    "X_offset_train_val_1 = X_offset_high_snr[:221]\n",
    "Y_error_train_val_1 = Y_error_high_snr[:221]\n",
    "\n",
    "X_train_val_2 = X_high_snr[221:721]\n",
    "Y_train_val_2 = Y_high_snr[221:721]\n",
    "K_mag_train_val_2 = K_mag_high_snr[221:721]\n",
    "X_offset_train_val_2 = X_offset_high_snr[221:721]\n",
    "Y_error_train_val_2 = Y_error_high_snr[221:721]\n",
    "\n",
    "X_train_val_3 = X_high_snr[721:1721]\n",
    "Y_train_val_3 = Y_high_snr[721:1721]\n",
    "K_mag_train_val_3 = K_mag_high_snr[721:1721]\n",
    "X_offset_train_val_3 = X_offset_high_snr[721:1721]\n",
    "Y_error_train_val_3 = Y_error_high_snr[721:1721]\n",
    "\n",
    "X_train_val_4 = X_high_snr[1721:4721]\n",
    "Y_train_val_4 = Y_high_snr[1721:4721]\n",
    "K_mag_train_val_4 = K_mag_high_snr[1721:4721]\n",
    "X_offset_train_val_4 = X_offset_high_snr[1721:4721]\n",
    "Y_error_train_val_4 = Y_error_high_snr[1721:4721]\n",
    "\n",
    "X_train_val_5 = X_high_snr[4721:12721]\n",
    "Y_train_val_5 = Y_high_snr[4721:12721]\n",
    "K_mag_train_val_5 = K_mag_high_snr[4721:12721]\n",
    "X_offset_train_val_5 = X_offset_high_snr[4721:12721]\n",
    "Y_error_train_val_5 = Y_error_high_snr[4721:12721]\n",
    "\n",
    "X_train_val_6 = X_high_snr[12721:]\n",
    "Y_train_val_6 = Y_high_snr[12721:]\n",
    "K_mag_train_val_6 = K_mag_high_snr[12721:]\n",
    "X_offset_train_val_6 = X_offset_high_snr[12721:]\n",
    "Y_error_train_val_6 = Y_error_high_snr[12721:]\n",
    "\n",
    "X_train_val_7 = X_high_snr\n",
    "Y_train_val_7 = Y_high_snr\n",
    "K_mag_train_val_7 = K_mag_high_snr\n",
    "X_offset_train_val_7 = X_offset_high_snr\n",
    "Y_error_train_val_7 = Y_error_high_snr\n",
    "\n",
    "X_test = X_low_snr\n",
    "Y_test = Y_low_snr\n",
    "K_mag_test = K_mag_low_snr\n",
    "X_offset_test = X_offset_low_snr\n",
    "Y_error_test = Y_error_low_snr\n",
    "snr_test = snr[low_snr_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(221, 1) (500, 1) (1000, 1) (3000, 1) (8000, 1) (15000, 1) (27721, 1) (33265, 1)\n"
     ]
    }
   ],
   "source": [
    "print(Y_train_val_1.shape , Y_train_val_2.shape , Y_train_val_3.shape, Y_train_val_4.shape , \n",
    "      Y_train_val_5.shape,Y_train_val_6.shape ,Y_train_val_7.shape , Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "pseudo-lum-input (InputLayer)   (None, 7514, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 7512, 2)      8           pseudo-lum-input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 7512, 2)      8           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 3756, 2)      0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 3754, 4)      28          max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 3754, 4)      16          conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 1877, 4)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 7508)         0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          961152      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           8256        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "offset-input (InputLayer)       (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 32)           2080        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           256         offset-input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "pseudo-lum (Dense)              (None, 1)            33          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "K_mag (InputLayer)              (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 32)           2080        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "parallax (Lambda)               (None, 1)            0           pseudo-lum[0][0]                 \n",
      "                                                                 K_mag[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "offset (Dense)                  (None, 1)            33          dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum-parallax-offset (Lambda)    (None, 1)            0           parallax[0][0]                   \n",
      "                                                                 offset[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 973,950\n",
      "Trainable params: 973,938\n",
      "Non-trainable params: 12\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_timesteps, n_features = X_train_val_7.shape[1], X_train_val_7.shape[2]\n",
    "\n",
    "Global_model = ApogeeDR14GaiaDR2(n_timesteps , n_features)\n",
    "\n",
    "Global_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/200\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 7032.2049 - mse: 16.2338 - val_loss: 6534.3346 - val_mse: 11.3679\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 6534.33465, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 2/200\n",
      "800/800 [==============================] - 1s 922us/step - loss: 6447.4136 - mse: 15.1053 - val_loss: 5849.5833 - val_mse: 10.1787\n",
      "\n",
      "Epoch 00002: val_loss improved from 6534.33465 to 5849.58327, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 3/200\n",
      "800/800 [==============================] - 1s 992us/step - loss: 5488.9433 - mse: 13.0039 - val_loss: 5036.8604 - val_mse: 8.7642\n",
      "\n",
      "Epoch 00003: val_loss improved from 5849.58327 to 5036.86044, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 4/200\n",
      "800/800 [==============================] - 1s 881us/step - loss: 4517.8139 - mse: 10.5205 - val_loss: 4962.9453 - val_mse: 8.6272\n",
      "\n",
      "Epoch 00004: val_loss improved from 5036.86044 to 4962.94527, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 5/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 3973.5681 - mse: 9.4661 - val_loss: 4816.6289 - val_mse: 8.3655\n",
      "\n",
      "Epoch 00005: val_loss improved from 4962.94527 to 4816.62894, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 6/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 3361.4789 - mse: 8.0862 - val_loss: 4770.2708 - val_mse: 8.2810\n",
      "\n",
      "Epoch 00006: val_loss improved from 4816.62894 to 4770.27079, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 7/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 2846.3581 - mse: 6.8788 - val_loss: 4762.5603 - val_mse: 8.2585\n",
      "\n",
      "Epoch 00007: val_loss improved from 4770.27079 to 4762.56031, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 8/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 2243.3801 - mse: 5.6114 - val_loss: 4757.0080 - val_mse: 8.2425\n",
      "\n",
      "Epoch 00008: val_loss improved from 4762.56031 to 4757.00797, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 9/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 1845.8232 - mse: 4.6903 - val_loss: 4831.1503 - val_mse: 8.3688\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 4757.00797\n",
      "Epoch 10/200\n",
      "800/800 [==============================] - 1s 835us/step - loss: 1558.5738 - mse: 4.0228 - val_loss: 4833.5387 - val_mse: 8.3756\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 4757.00797\n",
      "Epoch 11/200\n",
      "800/800 [==============================] - 1s 875us/step - loss: 1169.8914 - mse: 3.2487 - val_loss: 4831.7233 - val_mse: 8.3771\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 4757.00797\n",
      "Epoch 12/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 941.6517 - mse: 2.6405 - val_loss: 4891.0238 - val_mse: 8.4858\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 4757.00797\n",
      "Epoch 13/200\n",
      "800/800 [==============================] - 1s 823us/step - loss: 680.4237 - mse: 2.0533 - val_loss: 4852.9462 - val_mse: 8.4241\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 4757.00797\n",
      "Epoch 14/200\n",
      "800/800 [==============================] - 1s 971us/step - loss: 567.6915 - mse: 1.7398 - val_loss: 4765.6698 - val_mse: 8.2739\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 4757.00797\n",
      "Epoch 15/200\n",
      "800/800 [==============================] - 1s 831us/step - loss: 473.0959 - mse: 1.3781 - val_loss: 4803.2761 - val_mse: 8.3409\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 4757.00797\n",
      "Epoch 16/200\n",
      "800/800 [==============================] - 1s 808us/step - loss: 408.3968 - mse: 1.2336 - val_loss: 4730.1037 - val_mse: 8.2159\n",
      "\n",
      "Epoch 00016: val_loss improved from 4757.00797 to 4730.10365, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 17/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 375.3835 - mse: 1.0242 - val_loss: 4731.5047 - val_mse: 8.2203\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 4730.10365\n",
      "Epoch 18/200\n",
      "800/800 [==============================] - 1s 834us/step - loss: 301.4946 - mse: 0.8724 - val_loss: 4690.5669 - val_mse: 8.1518\n",
      "\n",
      "Epoch 00018: val_loss improved from 4730.10365 to 4690.56690, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 19/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 254.6075 - mse: 0.7140 - val_loss: 4668.2922 - val_mse: 8.1161\n",
      "\n",
      "Epoch 00019: val_loss improved from 4690.56690 to 4668.29221, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 20/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 217.7341 - mse: 0.6356 - val_loss: 4665.7519 - val_mse: 8.1136\n",
      "\n",
      "Epoch 00020: val_loss improved from 4668.29221 to 4665.75192, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 21/200\n",
      "800/800 [==============================] - 1s 934us/step - loss: 168.9845 - mse: 0.4894 - val_loss: 4629.0522 - val_mse: 8.0544\n",
      "\n",
      "Epoch 00021: val_loss improved from 4665.75192 to 4629.05221, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 22/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 149.0470 - mse: 0.4230 - val_loss: 4617.7630 - val_mse: 8.0375\n",
      "\n",
      "Epoch 00022: val_loss improved from 4629.05221 to 4617.76300, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 23/200\n",
      "800/800 [==============================] - 1s 836us/step - loss: 125.0670 - mse: 0.3719 - val_loss: 4599.2574 - val_mse: 8.0087\n",
      "\n",
      "Epoch 00023: val_loss improved from 4617.76300 to 4599.25736, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 24/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 109.2124 - mse: 0.3191 - val_loss: 4582.4755 - val_mse: 7.9833\n",
      "\n",
      "Epoch 00024: val_loss improved from 4599.25736 to 4582.47551, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 25/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 92.4435 - mse: 0.2835 - val_loss: 4564.7311 - val_mse: 7.9570\n",
      "\n",
      "Epoch 00025: val_loss improved from 4582.47551 to 4564.73111, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 26/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 84.5204 - mse: 0.2714 - val_loss: 4549.4165 - val_mse: 7.9344\n",
      "\n",
      "Epoch 00026: val_loss improved from 4564.73111 to 4549.41646, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 27/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 76.2186 - mse: 0.2338 - val_loss: 4535.8167 - val_mse: 7.9155\n",
      "\n",
      "Epoch 00027: val_loss improved from 4549.41646 to 4535.81673, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 28/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 65.7625 - mse: 0.2134 - val_loss: 4521.6528 - val_mse: 7.8958\n",
      "\n",
      "Epoch 00028: val_loss improved from 4535.81673 to 4521.65283, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 29/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 1s 1ms/step - loss: 59.4720 - mse: 0.1921 - val_loss: 4509.2750 - val_mse: 7.8790\n",
      "\n",
      "Epoch 00029: val_loss improved from 4521.65283 to 4509.27502, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 30/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 53.6533 - mse: 0.1758 - val_loss: 4501.7945 - val_mse: 7.8715\n",
      "\n",
      "Epoch 00030: val_loss improved from 4509.27502 to 4501.79451, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 31/200\n",
      "800/800 [==============================] - 1s 931us/step - loss: 48.8333 - mse: 0.1619 - val_loss: 4491.4057 - val_mse: 7.8588\n",
      "\n",
      "Epoch 00031: val_loss improved from 4501.79451 to 4491.40566, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 32/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 43.4402 - mse: 0.1433 - val_loss: 4485.4860 - val_mse: 7.8533\n",
      "\n",
      "Epoch 00032: val_loss improved from 4491.40566 to 4485.48601, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 33/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 38.7431 - mse: 0.1294 - val_loss: 4481.7121 - val_mse: 7.8521\n",
      "\n",
      "Epoch 00033: val_loss improved from 4485.48601 to 4481.71206, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 34/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 34.3478 - mse: 0.1165 - val_loss: 4475.3786 - val_mse: 7.8459\n",
      "\n",
      "Epoch 00034: val_loss improved from 4481.71206 to 4475.37864, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 35/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 31.8334 - mse: 0.1070 - val_loss: 4477.5510 - val_mse: 7.8554\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 4475.37864\n",
      "Epoch 36/200\n",
      "800/800 [==============================] - 1s 825us/step - loss: 28.2630 - mse: 0.0993 - val_loss: 4478.5438 - val_mse: 7.8622\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 4475.37864\n",
      "Epoch 37/200\n",
      "800/800 [==============================] - 1s 843us/step - loss: 25.7889 - mse: 0.0887 - val_loss: 4481.1067 - val_mse: 7.8714\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 4475.37864\n",
      "Epoch 38/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 23.3535 - mse: 0.0813 - val_loss: 4478.3262 - val_mse: 7.8702\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 4475.37864\n",
      "Epoch 39/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 21.3067 - mse: 0.0732 - val_loss: 4488.5245 - val_mse: 7.8929\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 4475.37864\n",
      "Epoch 40/200\n",
      "800/800 [==============================] - 1s 991us/step - loss: 19.4122 - mse: 0.0694 - val_loss: 4478.5983 - val_mse: 7.8761\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 4475.37864\n",
      "Epoch 41/200\n",
      "800/800 [==============================] - 1s 951us/step - loss: 18.2058 - mse: 0.0640 - val_loss: 4487.3634 - val_mse: 7.8933\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 4475.37864\n",
      "Epoch 42/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 16.9310 - mse: 0.0625 - val_loss: 4486.0548 - val_mse: 7.8905\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 4475.37864\n",
      "Epoch 43/200\n",
      "800/800 [==============================] - 1s 960us/step - loss: 15.4434 - mse: 0.0578 - val_loss: 4488.6329 - val_mse: 7.8935\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 4475.37864\n",
      "Epoch 44/200\n",
      "800/800 [==============================] - 1s 880us/step - loss: 14.5137 - mse: 0.0554 - val_loss: 4487.6184 - val_mse: 7.8894\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 4475.37864\n",
      "Epoch 45/200\n",
      "800/800 [==============================] - 1s 848us/step - loss: 13.5627 - mse: 0.0531 - val_loss: 4487.1688 - val_mse: 7.8841\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 4475.37864\n",
      "Epoch 46/200\n",
      "800/800 [==============================] - 1s 950us/step - loss: 13.1615 - mse: 0.0517 - val_loss: 4482.1845 - val_mse: 7.8697\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 4475.37864\n",
      "Epoch 47/200\n",
      "800/800 [==============================] - 1s 855us/step - loss: 12.6899 - mse: 0.0506 - val_loss: 4475.5630 - val_mse: 7.8511\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 4475.37864\n",
      "Epoch 48/200\n",
      "800/800 [==============================] - 1s 797us/step - loss: 12.1908 - mse: 0.0492 - val_loss: 4461.7481 - val_mse: 7.8174\n",
      "\n",
      "Epoch 00048: val_loss improved from 4475.37864 to 4461.74810, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 49/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 11.8416 - mse: 0.0480 - val_loss: 4450.6773 - val_mse: 7.7880\n",
      "\n",
      "Epoch 00049: val_loss improved from 4461.74810 to 4450.67733, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 50/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 11.3823 - mse: 0.0468 - val_loss: 4434.0294 - val_mse: 7.7481\n",
      "\n",
      "Epoch 00050: val_loss improved from 4450.67733 to 4434.02939, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 51/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 11.0780 - mse: 0.0459 - val_loss: 4402.5969 - val_mse: 7.6797\n",
      "\n",
      "Epoch 00051: val_loss improved from 4434.02939 to 4402.59689, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 52/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 10.6284 - mse: 0.0445 - val_loss: 4376.6369 - val_mse: 7.6203\n",
      "\n",
      "Epoch 00052: val_loss improved from 4402.59689 to 4376.63686, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 53/200\n",
      "800/800 [==============================] - 1s 976us/step - loss: 10.2321 - mse: 0.0436 - val_loss: 4344.8853 - val_mse: 7.5505\n",
      "\n",
      "Epoch 00053: val_loss improved from 4376.63686 to 4344.88531, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 54/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 9.8793 - mse: 0.0427 - val_loss: 4303.2759 - val_mse: 7.4630\n",
      "\n",
      "Epoch 00054: val_loss improved from 4344.88531 to 4303.27594, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 55/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 9.5932 - mse: 0.0414 - val_loss: 4257.7412 - val_mse: 7.3701\n",
      "\n",
      "Epoch 00055: val_loss improved from 4303.27594 to 4257.74123, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 56/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 9.2818 - mse: 0.0407 - val_loss: 4206.0241 - val_mse: 7.2676\n",
      "\n",
      "Epoch 00056: val_loss improved from 4257.74123 to 4206.02406, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 57/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 8.9726 - mse: 0.0398 - val_loss: 4151.5417 - val_mse: 7.1606\n",
      "\n",
      "Epoch 00057: val_loss improved from 4206.02406 to 4151.54165, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 58/200\n",
      "800/800 [==============================] - 1s 919us/step - loss: 8.4843 - mse: 0.0384 - val_loss: 4088.6967 - val_mse: 7.0401\n",
      "\n",
      "Epoch 00058: val_loss improved from 4151.54165 to 4088.69674, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 1s 1ms/step - loss: 8.1573 - mse: 0.0374 - val_loss: 4028.6193 - val_mse: 6.9259\n",
      "\n",
      "Epoch 00059: val_loss improved from 4088.69674 to 4028.61934, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 60/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 7.9412 - mse: 0.0367 - val_loss: 3954.7139 - val_mse: 6.7902\n",
      "\n",
      "Epoch 00060: val_loss improved from 4028.61934 to 3954.71395, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 61/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 7.6153 - mse: 0.0357 - val_loss: 3883.7022 - val_mse: 6.6631\n",
      "\n",
      "Epoch 00061: val_loss improved from 3954.71395 to 3883.70218, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 62/200\n",
      "800/800 [==============================] - 1s 995us/step - loss: 7.2933 - mse: 0.0348 - val_loss: 3808.6336 - val_mse: 6.5311\n",
      "\n",
      "Epoch 00062: val_loss improved from 3883.70218 to 3808.63360, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 63/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 7.0567 - mse: 0.0339 - val_loss: 3713.8485 - val_mse: 6.3700\n",
      "\n",
      "Epoch 00063: val_loss improved from 3808.63360 to 3713.84852, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 64/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 6.7745 - mse: 0.0330 - val_loss: 3632.4052 - val_mse: 6.2346\n",
      "\n",
      "Epoch 00064: val_loss improved from 3713.84852 to 3632.40521, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 65/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 6.4708 - mse: 0.0325 - val_loss: 3542.4142 - val_mse: 6.0895\n",
      "\n",
      "Epoch 00065: val_loss improved from 3632.40521 to 3542.41424, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 66/200\n",
      "800/800 [==============================] - 1s 948us/step - loss: 6.1873 - mse: 0.0314 - val_loss: 3437.1208 - val_mse: 5.9224\n",
      "\n",
      "Epoch 00066: val_loss improved from 3542.41424 to 3437.12084, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 67/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 5.9584 - mse: 0.0307 - val_loss: 3344.9608 - val_mse: 5.7784\n",
      "\n",
      "Epoch 00067: val_loss improved from 3437.12084 to 3344.96084, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 68/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 5.7332 - mse: 0.0301 - val_loss: 3245.0347 - val_mse: 5.6215\n",
      "\n",
      "Epoch 00068: val_loss improved from 3344.96084 to 3245.03471, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 69/200\n",
      "800/800 [==============================] - 1s 931us/step - loss: 5.5122 - mse: 0.0293 - val_loss: 3133.6385 - val_mse: 5.4477\n",
      "\n",
      "Epoch 00069: val_loss improved from 3245.03471 to 3133.63849, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 70/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 5.2704 - mse: 0.0286 - val_loss: 3028.3828 - val_mse: 5.2819\n",
      "\n",
      "Epoch 00070: val_loss improved from 3133.63849 to 3028.38277, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 71/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 5.1099 - mse: 0.0281 - val_loss: 2923.0083 - val_mse: 5.1144\n",
      "\n",
      "Epoch 00071: val_loss improved from 3028.38277 to 2923.00825, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 72/200\n",
      "800/800 [==============================] - 1s 800us/step - loss: 4.8654 - mse: 0.0274 - val_loss: 2797.8724 - val_mse: 4.9121\n",
      "\n",
      "Epoch 00072: val_loss improved from 2923.00825 to 2797.87242, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 73/200\n",
      "800/800 [==============================] - 1s 836us/step - loss: 4.6696 - mse: 0.0266 - val_loss: 2684.8357 - val_mse: 4.7275\n",
      "\n",
      "Epoch 00073: val_loss improved from 2797.87242 to 2684.83566, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 74/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 4.4824 - mse: 0.0262 - val_loss: 2560.6705 - val_mse: 4.5232\n",
      "\n",
      "Epoch 00074: val_loss improved from 2684.83566 to 2560.67053, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 75/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 4.3293 - mse: 0.0256 - val_loss: 2432.4091 - val_mse: 4.3119\n",
      "\n",
      "Epoch 00075: val_loss improved from 2560.67053 to 2432.40906, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 76/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 4.0785 - mse: 0.0248 - val_loss: 2318.1666 - val_mse: 4.1215\n",
      "\n",
      "Epoch 00076: val_loss improved from 2432.40906 to 2318.16656, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 77/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 3.9141 - mse: 0.0244 - val_loss: 2178.6881 - val_mse: 3.8894\n",
      "\n",
      "Epoch 00077: val_loss improved from 2318.16656 to 2178.68810, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 78/200\n",
      "800/800 [==============================] - 1s 901us/step - loss: 3.7329 - mse: 0.0238 - val_loss: 2037.8611 - val_mse: 3.6524\n",
      "\n",
      "Epoch 00078: val_loss improved from 2178.68810 to 2037.86110, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 79/200\n",
      "800/800 [==============================] - 1s 889us/step - loss: 3.6257 - mse: 0.0233 - val_loss: 1911.0226 - val_mse: 3.4381\n",
      "\n",
      "Epoch 00079: val_loss improved from 2037.86110 to 1911.02261, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 80/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 3.3957 - mse: 0.0227 - val_loss: 1783.7334 - val_mse: 3.2219\n",
      "\n",
      "Epoch 00080: val_loss improved from 1911.02261 to 1783.73345, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 81/200\n",
      "800/800 [==============================] - 1s 954us/step - loss: 3.2885 - mse: 0.0223 - val_loss: 1641.2821 - val_mse: 2.9822\n",
      "\n",
      "Epoch 00081: val_loss improved from 1783.73345 to 1641.28210, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 82/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 3.1209 - mse: 0.0217 - val_loss: 1525.9966 - val_mse: 2.7871\n",
      "\n",
      "Epoch 00082: val_loss improved from 1641.28210 to 1525.99663, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 83/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 2.9861 - mse: 0.0212 - val_loss: 1392.3758 - val_mse: 2.5633\n",
      "\n",
      "Epoch 00083: val_loss improved from 1525.99663 to 1392.37578, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 84/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 2.8322 - mse: 0.0208 - val_loss: 1276.8144 - val_mse: 2.3698\n",
      "\n",
      "Epoch 00084: val_loss improved from 1392.37578 to 1276.81443, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 2.6837 - mse: 0.0204 - val_loss: 1172.3000 - val_mse: 2.1939\n",
      "\n",
      "Epoch 00085: val_loss improved from 1276.81443 to 1172.30002, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 86/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 2.5614 - mse: 0.0200 - val_loss: 1074.7958 - val_mse: 2.0296\n",
      "\n",
      "Epoch 00086: val_loss improved from 1172.30002 to 1074.79583, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 87/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 2.4743 - mse: 0.0196 - val_loss: 969.3518 - val_mse: 1.8526\n",
      "\n",
      "Epoch 00087: val_loss improved from 1074.79583 to 969.35176, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 88/200\n",
      "800/800 [==============================] - 1s 926us/step - loss: 2.3994 - mse: 0.0194 - val_loss: 896.9094 - val_mse: 1.7311\n",
      "\n",
      "Epoch 00088: val_loss improved from 969.35176 to 896.90937, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 89/200\n",
      "800/800 [==============================] - 1s 982us/step - loss: 2.2690 - mse: 0.0189 - val_loss: 833.5326 - val_mse: 1.6263\n",
      "\n",
      "Epoch 00089: val_loss improved from 896.90937 to 833.53259, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 90/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 2.1461 - mse: 0.0186 - val_loss: 769.0443 - val_mse: 1.5174\n",
      "\n",
      "Epoch 00090: val_loss improved from 833.53259 to 769.04426, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 91/200\n",
      "800/800 [==============================] - 1s 810us/step - loss: 2.0788 - mse: 0.0183 - val_loss: 716.5261 - val_mse: 1.4277\n",
      "\n",
      "Epoch 00091: val_loss improved from 769.04426 to 716.52614, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 92/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 1.9183 - mse: 0.0178 - val_loss: 679.1255 - val_mse: 1.3618\n",
      "\n",
      "Epoch 00092: val_loss improved from 716.52614 to 679.12551, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 93/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 1.7529 - mse: 0.0174 - val_loss: 644.0081 - val_mse: 1.2992\n",
      "\n",
      "Epoch 00093: val_loss improved from 679.12551 to 644.00814, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 94/200\n",
      "800/800 [==============================] - 1s 959us/step - loss: 1.6444 - mse: 0.0170 - val_loss: 616.3178 - val_mse: 1.2484\n",
      "\n",
      "Epoch 00094: val_loss improved from 644.00814 to 616.31779, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 95/200\n",
      "800/800 [==============================] - 1s 943us/step - loss: 1.5395 - mse: 0.0167 - val_loss: 592.2906 - val_mse: 1.2028\n",
      "\n",
      "Epoch 00095: val_loss improved from 616.31779 to 592.29062, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 96/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 1.4453 - mse: 0.0164 - val_loss: 575.5523 - val_mse: 1.1692\n",
      "\n",
      "Epoch 00096: val_loss improved from 592.29062 to 575.55226, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 97/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 1.2979 - mse: 0.0160 - val_loss: 562.2426 - val_mse: 1.1405\n",
      "\n",
      "Epoch 00097: val_loss improved from 575.55226 to 562.24265, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 98/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 1.2164 - mse: 0.0157 - val_loss: 551.6740 - val_mse: 1.1167\n",
      "\n",
      "Epoch 00098: val_loss improved from 562.24265 to 551.67405, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 99/200\n",
      "800/800 [==============================] - 1s 938us/step - loss: 1.1197 - mse: 0.0154 - val_loss: 545.7423 - val_mse: 1.0995\n",
      "\n",
      "Epoch 00099: val_loss improved from 551.67405 to 545.74228, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 100/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 1.0912 - mse: 0.0152 - val_loss: 539.5699 - val_mse: 1.0823\n",
      "\n",
      "Epoch 00100: val_loss improved from 545.74228 to 539.56987, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 101/200\n",
      "800/800 [==============================] - 1s 866us/step - loss: 0.9563 - mse: 0.0149 - val_loss: 536.7739 - val_mse: 1.0709\n",
      "\n",
      "Epoch 00101: val_loss improved from 539.56987 to 536.77392, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 102/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.8910 - mse: 0.0146 - val_loss: 536.5848 - val_mse: 1.0659\n",
      "\n",
      "Epoch 00102: val_loss improved from 536.77392 to 536.58478, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 103/200\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.8180 - mse: 0.0144 - val_loss: 536.5727 - val_mse: 1.0591\n",
      "\n",
      "Epoch 00103: val_loss improved from 536.58478 to 536.57272, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5\n",
      "Epoch 104/200\n",
      "800/800 [==============================] - 1s 891us/step - loss: 0.7083 - mse: 0.0140 - val_loss: 538.0776 - val_mse: 1.0560\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 536.57272\n",
      "Epoch 105/200\n",
      "800/800 [==============================] - 1s 809us/step - loss: 0.6692 - mse: 0.0139 - val_loss: 540.1760 - val_mse: 1.0540\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 536.57272\n",
      "Epoch 106/200\n",
      "800/800 [==============================] - 1s 812us/step - loss: 0.5567 - mse: 0.0137 - val_loss: 543.3638 - val_mse: 1.0530\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 536.57272\n",
      "Epoch 107/200\n",
      "800/800 [==============================] - 1s 805us/step - loss: 0.4729 - mse: 0.0132 - val_loss: 547.2703 - val_mse: 1.0555\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 536.57272\n",
      "Epoch 108/200\n",
      "800/800 [==============================] - 1s 812us/step - loss: 0.3933 - mse: 0.0131 - val_loss: 549.8670 - val_mse: 1.0548\n",
      "\n",
      "Epoch 00108: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 536.57272\n",
      "Epoch 109/200\n",
      "800/800 [==============================] - 1s 809us/step - loss: 0.3287 - mse: 0.0129 - val_loss: 551.8524 - val_mse: 1.0541\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 536.57272\n",
      "Epoch 110/200\n",
      "800/800 [==============================] - 1s 808us/step - loss: 0.2873 - mse: 0.0128 - val_loss: 555.8027 - val_mse: 1.0566\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 536.57272\n",
      "Epoch 111/200\n",
      "800/800 [==============================] - 1s 822us/step - loss: 0.2542 - mse: 0.0127 - val_loss: 557.7893 - val_mse: 1.0561\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 536.57272\n",
      "Epoch 112/200\n",
      "800/800 [==============================] - 1s 815us/step - loss: 0.2142 - mse: 0.0125 - val_loss: 562.4473 - val_mse: 1.0611\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 536.57272\n",
      "Epoch 113/200\n",
      "800/800 [==============================] - 1s 810us/step - loss: 0.1917 - mse: 0.0125 - val_loss: 563.5692 - val_mse: 1.0602\n",
      "\n",
      "Epoch 00113: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 536.57272\n",
      "Epoch 114/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 1s 813us/step - loss: 0.1336 - mse: 0.0123 - val_loss: 567.0873 - val_mse: 1.0636\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 536.57272\n",
      "Epoch 115/200\n",
      "800/800 [==============================] - 1s 805us/step - loss: 0.1203 - mse: 0.0123 - val_loss: 568.2633 - val_mse: 1.0631\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 536.57272\n",
      "Epoch 116/200\n",
      "800/800 [==============================] - 1s 802us/step - loss: 0.1065 - mse: 0.0122 - val_loss: 570.5257 - val_mse: 1.0649\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 536.57272\n",
      "Epoch 117/200\n",
      "800/800 [==============================] - 1s 797us/step - loss: 0.0959 - mse: 0.0122 - val_loss: 571.6690 - val_mse: 1.0648\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 536.57272\n",
      "Epoch 118/200\n",
      "800/800 [==============================] - 1s 805us/step - loss: 0.0658 - mse: 0.0121 - val_loss: 572.8851 - val_mse: 1.0649\n",
      "\n",
      "Epoch 00118: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 536.57272\n",
      "Epoch 119/200\n",
      "800/800 [==============================] - 1s 801us/step - loss: 0.0480 - mse: 0.0121 - val_loss: 575.2611 - val_mse: 1.0671\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 536.57272\n",
      "Epoch 120/200\n",
      "800/800 [==============================] - 1s 802us/step - loss: 0.0506 - mse: 0.0121 - val_loss: 576.5972 - val_mse: 1.0678\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 536.57272\n",
      "Epoch 121/200\n",
      "800/800 [==============================] - 1s 814us/step - loss: 0.0329 - mse: 0.0120 - val_loss: 577.7959 - val_mse: 1.0684\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 536.57272\n",
      "Epoch 122/200\n",
      "800/800 [==============================] - 1s 810us/step - loss: 0.0307 - mse: 0.0120 - val_loss: 579.1925 - val_mse: 1.0695\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 536.57272\n",
      "Epoch 123/200\n",
      "800/800 [==============================] - 1s 805us/step - loss: 0.0150 - mse: 0.0120 - val_loss: 580.8313 - val_mse: 1.0711\n",
      "\n",
      "Epoch 00123: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 536.57272\n",
      "Epoch 124/200\n",
      "800/800 [==============================] - 1s 809us/step - loss: 0.0061 - mse: 0.0119 - val_loss: 582.2842 - val_mse: 1.0725\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 536.57272\n",
      "Epoch 125/200\n",
      "800/800 [==============================] - 1s 795us/step - loss: 0.0105 - mse: 0.0120 - val_loss: 583.2797 - val_mse: 1.0732\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 536.57272\n",
      "Epoch 126/200\n",
      "800/800 [==============================] - 1s 803us/step - loss: 0.0031 - mse: 0.0119 - val_loss: 583.9696 - val_mse: 1.0734\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 536.57272\n",
      "Epoch 127/200\n",
      "800/800 [==============================] - 1s 798us/step - loss: -0.0035 - mse: 0.0119 - val_loss: 585.1690 - val_mse: 1.0746\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 536.57272\n",
      "Epoch 128/200\n",
      "800/800 [==============================] - 1s 805us/step - loss: -0.0045 - mse: 0.0119 - val_loss: 586.6505 - val_mse: 1.0764\n",
      "\n",
      "Epoch 00128: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 536.57272\n",
      "Epoch 129/200\n",
      "800/800 [==============================] - 1s 800us/step - loss: -0.0082 - mse: 0.0119 - val_loss: 587.4084 - val_mse: 1.0770\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 536.57272\n",
      "Epoch 130/200\n",
      "800/800 [==============================] - 1s 815us/step - loss: -0.0111 - mse: 0.0119 - val_loss: 588.0404 - val_mse: 1.0775\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 536.57272\n",
      "Epoch 131/200\n",
      "800/800 [==============================] - 1s 808us/step - loss: -0.0138 - mse: 0.0119 - val_loss: 588.7615 - val_mse: 1.0782\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 536.57272\n",
      "Epoch 132/200\n",
      "800/800 [==============================] - 1s 834us/step - loss: -0.0187 - mse: 0.0119 - val_loss: 589.7665 - val_mse: 1.0795\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 536.57272\n",
      "Epoch 133/200\n",
      "800/800 [==============================] - 1s 835us/step - loss: -0.0182 - mse: 0.0119 - val_loss: 590.4279 - val_mse: 1.0802\n",
      "\n",
      "Epoch 00133: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 536.57272\n",
      "Epoch 134/200\n",
      "800/800 [==============================] - 1s 828us/step - loss: -0.0201 - mse: 0.0119 - val_loss: 591.0933 - val_mse: 1.0810\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 536.57272\n",
      "Epoch 135/200\n",
      "800/800 [==============================] - 1s 827us/step - loss: -0.0214 - mse: 0.0119 - val_loss: 591.5940 - val_mse: 1.0815\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 536.57272\n",
      "Epoch 136/200\n",
      "800/800 [==============================] - 1s 811us/step - loss: -0.0227 - mse: 0.0119 - val_loss: 592.0786 - val_mse: 1.0820\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 536.57272\n",
      "Epoch 137/200\n",
      "800/800 [==============================] - 1s 839us/step - loss: -0.0238 - mse: 0.0119 - val_loss: 592.5352 - val_mse: 1.0826\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 536.57272\n",
      "Epoch 138/200\n",
      "800/800 [==============================] - 1s 807us/step - loss: -0.0252 - mse: 0.0119 - val_loss: 593.0290 - val_mse: 1.0832\n",
      "\n",
      "Epoch 00138: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 536.57272\n",
      "Epoch 139/200\n",
      "800/800 [==============================] - 1s 806us/step - loss: -0.0263 - mse: 0.0118 - val_loss: 593.5547 - val_mse: 1.0839\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 536.57272\n",
      "Epoch 140/200\n",
      "800/800 [==============================] - 1s 813us/step - loss: -0.0268 - mse: 0.0118 - val_loss: 593.9935 - val_mse: 1.0845\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 536.57272\n",
      "Epoch 141/200\n",
      "800/800 [==============================] - 1s 803us/step - loss: -0.0274 - mse: 0.0118 - val_loss: 594.4388 - val_mse: 1.0851\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 536.57272\n",
      "Epoch 142/200\n",
      "800/800 [==============================] - 1s 806us/step - loss: -0.0280 - mse: 0.0118 - val_loss: 594.7792 - val_mse: 1.0855\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 536.57272\n",
      "Epoch 143/200\n",
      "800/800 [==============================] - 1s 809us/step - loss: -0.0286 - mse: 0.0118 - val_loss: 595.1327 - val_mse: 1.0860\n",
      "\n",
      "Epoch 00143: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 536.57272\n",
      "Epoch 144/200\n",
      "800/800 [==============================] - 1s 802us/step - loss: -0.0292 - mse: 0.0118 - val_loss: 595.4238 - val_mse: 1.0864\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 536.57272\n",
      "Epoch 145/200\n",
      "800/800 [==============================] - 1s 811us/step - loss: -0.0295 - mse: 0.0118 - val_loss: 595.6917 - val_mse: 1.0868\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 536.57272\n",
      "Epoch 146/200\n",
      "800/800 [==============================] - 1s 811us/step - loss: -0.0298 - mse: 0.0118 - val_loss: 595.8836 - val_mse: 1.0871\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 536.57272\n",
      "Epoch 147/200\n",
      "800/800 [==============================] - 1s 802us/step - loss: -0.0301 - mse: 0.0118 - val_loss: 596.0783 - val_mse: 1.0874\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 536.57272\n",
      "Epoch 148/200\n",
      "800/800 [==============================] - 1s 803us/step - loss: -0.0304 - mse: 0.0118 - val_loss: 596.2529 - val_mse: 1.0877\n",
      "\n",
      "Epoch 00148: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 536.57272\n",
      "Epoch 149/200\n",
      "800/800 [==============================] - 1s 830us/step - loss: -0.0307 - mse: 0.0118 - val_loss: 596.4229 - val_mse: 1.0879\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 536.57272\n",
      "Epoch 150/200\n",
      "800/800 [==============================] - 1s 825us/step - loss: -0.0308 - mse: 0.0118 - val_loss: 596.5531 - val_mse: 1.0881\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 536.57272\n",
      "Epoch 151/200\n",
      "800/800 [==============================] - 1s 829us/step - loss: -0.0310 - mse: 0.0118 - val_loss: 596.7070 - val_mse: 1.0883\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 536.57272\n",
      "Epoch 152/200\n",
      "800/800 [==============================] - 1s 831us/step - loss: -0.0312 - mse: 0.0118 - val_loss: 596.8519 - val_mse: 1.0885\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 536.57272\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 1s 845us/step - loss: -0.0313 - mse: 0.0118 - val_loss: 596.9339 - val_mse: 1.0886\n",
      "\n",
      "Epoch 00153: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 536.57272\n",
      "Epoch 00153: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f1544649450>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Global_model.compile(optimizer='adam', loss= None, metrics=['mse'])\n",
    "\n",
    "path_local = \"/home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss\"\n",
    "\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=50, verbose=1, min_delta=1e-7)\n",
    "checkpoint = ModelCheckpoint(f'{path_local}/Modelo_1_D3_ReplicaBovy_Custom_Loss_2.h5', monitor='val_loss', \n",
    "                             verbose=1, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, verbose=1, patience=5, min_lr=0.000000001)\n",
    "\n",
    "callbacks=[reduce_lr, checkpoint, earlystopper]\n",
    "\n",
    "Global_model.fit([X_train_val_3, K_mag_train_val_3, X_offset_train_val_3,Y_error_train_val_3], Y_train_val_3,epochs=200, \n",
    "                 batch_size=128, verbose=1, shuffle=\"batch\" ,callbacks=callbacks,validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### simple evaluations on test sample (SNR < 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33265/33265 [==============================] - 11s 340us/step\n"
     ]
    }
   ],
   "source": [
    "J_test , mse_test = Global_model.evaluate([X_test, K_mag_test , X_offset_test, Y_error_test], Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49199.9336953878 328.180419921875\n"
     ]
    }
   ],
   "source": [
    "print(J_test,mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save weights and structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Global_model.save_weights(f'{path_local}/Weights_1_D3_ReplicaBovy_Custom_Loss_21.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"class_name\": \"Model\", \"config\": {\"name\": \"model_1\", \"layers\": [{\"name\": \"pseudo-lum-input\", \"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 7514, 1], \"dtype\": \"float32\", \"sparse\": false, \"name\": \"pseudo-lum-input\"}, \"inbound_nodes\": []}, {\"name\": \"conv1d_1\", \"class_name\": \"Conv1D\", \"config\": {\"name\": \"conv1d_1\", \"trainable\": true, \"dtype\": \"float32\", \"filters\": 2, \"kernel_size\": [3], \"strides\": [1], \"padding\": \"valid\", \"data_format\": \"channels_last\", \"dilation_rate\": [1], \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"VarianceScaling\", \"config\": {\"scale\": 1.0, \"mode\": \"fan_avg\", \"distribution\": \"uniform\", \"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"inbound_nodes\": [[[\"pseudo-lum-input\", 0, 0, {}]]]}, {\"name\": \"batch_normalization_1\", \"class_name\": \"BatchNormalization\", \"config\": {\"name\": \"batch_normalization_1\", \"trainable\": true, \"dtype\": \"float32\", \"axis\": -1, \"momentum\": 0.99, \"epsilon\": 0.001, \"center\": true, \"scale\": true, \"beta_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"gamma_initializer\": {\"class_name\": \"Ones\", \"config\": {}}, \"moving_mean_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"moving_variance_initializer\": {\"class_name\": \"Ones\", \"config\": {}}, \"beta_regularizer\": null, \"gamma_regularizer\": null, \"beta_constraint\": null, \"gamma_constraint\": null}, \"inbound_nodes\": [[[\"conv1d_1\", 0, 0, {}]]]}, {\"name\": \"max_pooling1d_1\", \"class_name\": \"MaxPooling1D\", \"config\": {\"name\": \"max_pooling1d_1\", \"trainable\": true, \"dtype\": \"float32\", \"strides\": [2], \"pool_size\": [2], \"padding\": \"valid\", \"data_format\": \"channels_last\"}, \"inbound_nodes\": [[[\"batch_normalization_1\", 0, 0, {}]]]}, {\"name\": \"conv1d_2\", \"class_name\": \"Conv1D\", \"config\": {\"name\": \"conv1d_2\", \"trainable\": true, \"dtype\": \"float32\", \"filters\": 4, \"kernel_size\": [3], \"strides\": [1], \"padding\": \"valid\", \"data_format\": \"channels_last\", \"dilation_rate\": [1], \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"VarianceScaling\", \"config\": {\"scale\": 1.0, \"mode\": \"fan_avg\", \"distribution\": \"uniform\", \"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"inbound_nodes\": [[[\"max_pooling1d_1\", 0, 0, {}]]]}, {\"name\": \"batch_normalization_2\", \"class_name\": \"BatchNormalization\", \"config\": {\"name\": \"batch_normalization_2\", \"trainable\": true, \"dtype\": \"float32\", \"axis\": -1, \"momentum\": 0.99, \"epsilon\": 0.001, \"center\": true, \"scale\": true, \"beta_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"gamma_initializer\": {\"class_name\": \"Ones\", \"config\": {}}, \"moving_mean_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"moving_variance_initializer\": {\"class_name\": \"Ones\", \"config\": {}}, \"beta_regularizer\": null, \"gamma_regularizer\": null, \"beta_constraint\": null, \"gamma_constraint\": null}, \"inbound_nodes\": [[[\"conv1d_2\", 0, 0, {}]]]}, {\"name\": \"max_pooling1d_2\", \"class_name\": \"MaxPooling1D\", \"config\": {\"name\": \"max_pooling1d_2\", \"trainable\": true, \"dtype\": \"float32\", \"strides\": [2], \"pool_size\": [2], \"padding\": \"valid\", \"data_format\": \"channels_last\"}, \"inbound_nodes\": [[[\"batch_normalization_2\", 0, 0, {}]]]}, {\"name\": \"flatten_1\", \"class_name\": \"Flatten\", \"config\": {\"name\": \"flatten_1\", \"trainable\": true, \"dtype\": \"float32\", \"data_format\": \"channels_last\"}, \"inbound_nodes\": [[[\"max_pooling1d_2\", 0, 0, {}]]]}, {\"name\": \"dense_1\", \"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 128, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"VarianceScaling\", \"config\": {\"scale\": 1.0, \"mode\": \"fan_avg\", \"distribution\": \"uniform\", \"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"inbound_nodes\": [[[\"flatten_1\", 0, 0, {}]]]}, {\"name\": \"dense_2\", \"class_name\": \"Dense\", \"config\": {\"name\": \"dense_2\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 64, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"VarianceScaling\", \"config\": {\"scale\": 1.0, \"mode\": \"fan_avg\", \"distribution\": \"uniform\", \"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"inbound_nodes\": [[[\"dense_1\", 0, 0, {}]]]}, {\"name\": \"offset-input\", \"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 3], \"dtype\": \"float32\", \"sparse\": false, \"name\": \"offset-input\"}, \"inbound_nodes\": []}, {\"name\": \"dense_3\", \"class_name\": \"Dense\", \"config\": {\"name\": \"dense_3\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 32, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"VarianceScaling\", \"config\": {\"scale\": 1.0, \"mode\": \"fan_avg\", \"distribution\": \"uniform\", \"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"inbound_nodes\": [[[\"dense_2\", 0, 0, {}]]]}, {\"name\": \"dense_4\", \"class_name\": \"Dense\", \"config\": {\"name\": \"dense_4\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 64, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"VarianceScaling\", \"config\": {\"scale\": 1.0, \"mode\": \"fan_avg\", \"distribution\": \"uniform\", \"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"inbound_nodes\": [[[\"offset-input\", 0, 0, {}]]]}, {\"name\": \"pseudo-lum\", \"class_name\": \"Dense\", \"config\": {\"name\": \"pseudo-lum\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"softplus\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"VarianceScaling\", \"config\": {\"scale\": 1.0, \"mode\": \"fan_avg\", \"distribution\": \"uniform\", \"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"inbound_nodes\": [[[\"dense_3\", 0, 0, {}]]]}, {\"name\": \"K_mag\", \"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 1], \"dtype\": \"float32\", \"sparse\": false, \"name\": \"K_mag\"}, \"inbound_nodes\": []}, {\"name\": \"dense_5\", \"class_name\": \"Dense\", \"config\": {\"name\": \"dense_5\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 32, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"VarianceScaling\", \"config\": {\"scale\": 1.0, \"mode\": \"fan_avg\", \"distribution\": \"uniform\", \"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"inbound_nodes\": [[[\"dense_4\", 0, 0, {}]]]}, {\"name\": \"parallax\", \"class_name\": \"Lambda\", \"config\": {\"name\": \"parallax\", \"trainable\": true, \"dtype\": \"float32\", \"function\": [\"4wEAAAAAAAAAAQAAAAsAAABTAAAAcyoAAAB0AGoBoAJ8AGQBGQB0AGoBoANkAnQAagGgAmQDfABk\\\\nBBkAoQKhAqECUwApBU7pAAAAAGcAAAAAAAAkQGeamZmZmZnJv+kBAAAAKQTaAnRm2gRtYXRo2ght\\\\ndWx0aXBsedoDcG93KQHaCGZ1bmN0aW9uqQByCAAAAPoePGlweXRob24taW5wdXQtMy1lMGVlZDRj\\\\nNzMxZDA+2gg8bGFtYmRhPi4AAABzAgAAABQB\\\\n\", null, null], \"function_type\": \"lambda\", \"output_shape\": null, \"output_shape_type\": \"raw\", \"arguments\": {}}, \"inbound_nodes\": [[[\"pseudo-lum\", 0, 0, {}], [\"K_mag\", 0, 0, {}]]]}, {\"name\": \"offset\", \"class_name\": \"Dense\", \"config\": {\"name\": \"offset\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"tanh\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"VarianceScaling\", \"config\": {\"scale\": 1.0, \"mode\": \"fan_avg\", \"distribution\": \"uniform\", \"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"inbound_nodes\": [[[\"dense_5\", 0, 0, {}]]]}, {\"name\": \"sum-parallax-offset\", \"class_name\": \"Lambda\", \"config\": {\"name\": \"sum-parallax-offset\", \"trainable\": true, \"dtype\": \"float32\", \"function\": [\"4wEAAAAAAAAAAQAAAAUAAABTAAAAcxYAAAB0AGoBoAJ8AGQBGQB8AGQCGQChAlMAKQNO6QAAAADp\\\\nAQAAACkD2gJ0ZtoEbWF0aNoDYWRkKQHaCGZ1bmN0aW9uqQByBwAAAPoePGlweXRob24taW5wdXQt\\\\nMy1lMGVlZDRjNzMxZDA+2gg8bGFtYmRhPjIAAADzAAAAAA==\\\\n\", null, null], \"function_type\": \"lambda\", \"output_shape\": null, \"output_shape_type\": \"raw\", \"arguments\": {}}, \"inbound_nodes\": [[[\"parallax\", 0, 0, {}], [\"offset\", 0, 0, {}]]]}], \"input_layers\": [[\"pseudo-lum-input\", 0, 0], [\"K_mag\", 0, 0], [\"offset-input\", 0, 0]], \"output_layers\": [[\"sum-parallax-offset\", 0, 0]]}, \"keras_version\": \"2.3.1\", \"backend\": \"tensorflow\"}'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Global_model.to_json(f'{path_local}/Json_1_D3_ReplicaBovy_Custom_Loss_21.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Global_model.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.read_json(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.to_csv(r'/home/anell/Desktop/Bovy/AnellExercises/Good_Models/CustomLoss/Json_1_D3_ReplicaBovy_Custom_Loss_21.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = Global_model.predict([X_test, K_mag_test , X_offset_test, Y_error_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(target, pred, c=\"g\", alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"target\")\n",
    "plt.ylabel(\"pred\")\n",
    "\n",
    "plt.ylim(0,100)\n",
    "plt.xlim(0,100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
