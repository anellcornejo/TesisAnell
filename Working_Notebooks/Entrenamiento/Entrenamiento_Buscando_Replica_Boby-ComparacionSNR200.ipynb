{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.layers import Conv1D , Dropout , Flatten , MaxPooling1D, Dense, Input, BatchNormalization\n",
    "from keras.layers.core import Lambda\n",
    "from keras.models import Model , load_model\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import h5py\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ApogeeDR14GaiaDR2(dim_t , dim_n): \n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    dim_t - number of time steps of spectrum \n",
    "    dim_n - number of features of spectrum\n",
    "    \"\"\"\n",
    "    \n",
    "    #SPECTRUM TO LUINOSITY\n",
    "    dim_1 = 1 # number of corrected magnitude for one example \n",
    "    units = 1 #number of final output for one example+\n",
    "    \n",
    "    inputs_spectra = Input(shape=(dim_t, dim_n), name=\"pseudo-lum-input\") \n",
    "    inputs_mag = Input(shape=(dim_1,), name=\"K_mag\")\n",
    "    \n",
    "    x_parallax = Conv1D(filters=2, kernel_size=3, activation='relu')(inputs_spectra)\n",
    "    x_parallax = BatchNormalization()(x_parallax)\n",
    "    x_parallax = MaxPooling1D(pool_size=2)(x_parallax)\n",
    "    \n",
    "    x_parallax = Conv1D(filters=4, kernel_size=3, activation='relu')(x_parallax)\n",
    "    x_parallax = BatchNormalization()(x_parallax)\n",
    "    x_parallax = MaxPooling1D(pool_size=2)(x_parallax)\n",
    "    \n",
    "    x_parallax = Flatten()(x_parallax)\n",
    "    x_parallax = Dense(128, activation='relu')(x_parallax) \n",
    "    x_parallax = Dense(64, activation='relu')(x_parallax) \n",
    "    x_parallax = Dense(32, activation='relu')(x_parallax)\n",
    "    x_parallax = Dense(units, activation='softplus', name=\"pseudo-lum\")(x_parallax) \n",
    "      \n",
    "    #OFFSET CORRECTION : (optimization)\n",
    "    inputs_offset = Input(shape=(3,), name=\"offset-input\")\n",
    "    x_offset = Dense(64, activation='relu')(inputs_offset)\n",
    "    x_offset = Dense(32, activation='relu')(x_offset) \n",
    "    x_offset = Dense(units, activation='tanh', name=\"offset\")(x_offset) \n",
    "    \n",
    "    #Functions\n",
    "    outputs_parallax = Lambda(lambda function: tf.math.multiply(function[0], tf.math.pow(10., \n",
    "                              tf.math.multiply(-0.2, function[1]))),\n",
    "                              name='parallax')([x_parallax, inputs_mag])\n",
    "    \n",
    "    outputs_parallax_with_offset = Lambda(lambda function: tf.math.add(function[0], function[1]),\n",
    "                                          name=\"sum-parallax-offset\")([outputs_parallax, x_offset]) \n",
    "    \n",
    "    #Model setup\n",
    "    model =  Model(inputs = [inputs_spectra,inputs_mag, inputs_offset],outputs = [outputs_parallax_with_offset])\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos\n",
    "path_local_data = '/home/anell/Desktop/Bovy/AnellExercises/Fits_files'\n",
    "#path_local_data = '/home/bapanes/Research-Now/local/astronn-local/apo-gaia/'\n",
    "\n",
    "with h5py.File(f'{path_local_data}/apogeedr14_gaiadr2_with_spectrum_probando_rendimiento_2.h5','r') as F:  \n",
    "    parallax = np.array(F['parallax'])\n",
    "    parallax_error = np.array(F['parallax_err'])\n",
    "    spectra = np.array(F['spectra'])\n",
    "    Kmag = np.array(F['corrected_magnitude_K'])\n",
    "    bp_rp = np.array(F['bp_rp'])\n",
    "    Gmag = np.array(F['phot_g_mean_mag'])\n",
    "    teff = np.array(F['NN_teff'])\n",
    "    apogee_id = np.array(F['APOGEE_ID'])\n",
    "    snr = np.array(F['SNR'])\n",
    "    fe_h = np.array(F['Fe/H'])\n",
    "    path_spectra = np.array(F['Path_spectra'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60986,), (60986, 7514), (60986,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallax.shape , spectra.shape , Kmag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establecemos las variables que entrarán a la red y corregimos sus dimensiones\n",
    "X = np.expand_dims(spectra, axis = 2)\n",
    "Y = np.expand_dims(parallax, axis = 1)\n",
    "K_mag = np.expand_dims(Kmag, axis = 1)\n",
    "\n",
    "# Normalizamos Gmag , el color (G_bp - G_rp) y teff\n",
    "Gmag_std = np.std(Gmag)\n",
    "Gmag_mean = np.mean(Gmag)\n",
    "Gmag_norm = (Gmag - Gmag_mean) / Gmag_std\n",
    "\n",
    "bp_rp_std = np.std(bp_rp)\n",
    "bp_rp_mean = np.mean(bp_rp)\n",
    "bp_rp_norm = (bp_rp - bp_rp_mean) / bp_rp_std\n",
    "\n",
    "teff_std = np.std(teff)\n",
    "teff_mean = np.mean(teff)\n",
    "teff_norm = (teff - teff_mean) / teff_std\n",
    "\n",
    "G_mag = np.expand_dims(Gmag_norm, axis=1)\n",
    "Bp_Rp = np.expand_dims(bp_rp_norm, axis=1)\n",
    "Teff = np.expand_dims(teff_norm, axis=1)\n",
    "\n",
    "X_offset = np.concatenate((G_mag, Bp_Rp , Teff), axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60986, 7514, 1) (60986, 1) (60986, 1) (60986, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape , Y.shape, K_mag.shape, X_offset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNR cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_snr_idx = []\n",
    "low_snr_idx = []\n",
    "\n",
    "for i in range(len(snr)):\n",
    "    if snr[i] >= 200:           \n",
    "        high_snr_idx.append(i)\n",
    "    else:\n",
    "        low_snr_idx.append(i)\n",
    "\n",
    "random.seed(10)\n",
    "random.shuffle(high_snr_idx)\n",
    "random.seed(60)\n",
    "random.shuffle(low_snr_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estos valores pueden ser comparados con Bovy train-test separation, \n",
    "\n",
    "Apart from taht we should be more clear about data preparation and evaluation\n",
    "\n",
    "get consistent and repeatable accuracy using train and validation approach <br>\n",
    "undestand the process of validation (draft about probability_distributions) <br> \n",
    "check the precision on the test data using baseline separation <br>\n",
    "compute learning curve, following Fig 1. of Nguyen et. al.<br>\n",
    "try with percentual mse, such that low and high parallax values weight the same <br>\n",
    "\n",
    "initially we can consider SNR>200 for train-valid and SNR<200 for test <br>\n",
    "understand why it is possible to make this separation and hope for consistent results <br>\n",
    "\n",
    "compare to Bovy using this data separation, using global precision and learning curve<br>\n",
    "\n",
    "How well are we doing with respect to Bovy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image(\"/home/bapanes/Research-Now/TesisAnell/Figures/learning_curve_genoma.png\")\n",
    "#Image(\"learning_curve_genoma.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR>200: 27721, else: 33265\n"
     ]
    }
   ],
   "source": [
    "print(\"SNR>200: %d, else: %d\"%(len(high_snr_idx), len(low_snr_idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diseño Experimental:\n",
    "\n",
    "Consideraré train (0.8) y valid (0.2)\n",
    "\n",
    "train_val_1 $\\rightarrow$  177 (train) + 44 (valid) = 221\n",
    "\n",
    "train_val_2 $\\rightarrow$  400 (train) + 100 (valid) = 500\n",
    "\n",
    "train_val_3 $\\rightarrow$  800 (train) + 200 (valid) = 1000\n",
    "\n",
    "train_val_4 $\\rightarrow$  2400 (train) + 600 (valid) = 3000\n",
    "\n",
    "train_val_5 $\\rightarrow$  6400 (train) + 1600 (valid) = 8000\n",
    "\n",
    "train_val_6 $\\rightarrow$  12000 (train) + 3000 (valid) = 15000\n",
    "\n",
    "train_val_7 $\\rightarrow$  22177 (train) + 5594 (valid) = 27771\n",
    "\n",
    "test $\\rightarrow$ 33265"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_high_snr = X[high_snr_idx]\n",
    "Y_high_snr = Y[high_snr_idx]\n",
    "K_mag_high_snr = K_mag[high_snr_idx]\n",
    "X_offset_high_snr = X_offset[high_snr_idx]\n",
    "\n",
    "X_low_snr = X[low_snr_idx]\n",
    "Y_low_snr = Y[low_snr_idx]\n",
    "K_mag_low_snr = K_mag[low_snr_idx]\n",
    "X_offset_low_snr = X_offset[low_snr_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val_1 = X_high_snr[:221]\n",
    "Y_train_val_1 = Y_high_snr[:221]\n",
    "K_mag_train_val_1 = K_mag_high_snr[:221]\n",
    "X_offset_train_val_1 = X_offset_high_snr[:221]\n",
    "\n",
    "X_train_val_2 = X_high_snr[221:721]\n",
    "Y_train_val_2 = Y_high_snr[221:721]\n",
    "K_mag_train_val_2 = K_mag_high_snr[221:721]\n",
    "X_offset_train_val_2 = X_offset_high_snr[221:721]\n",
    "\n",
    "X_train_val_3 = X_high_snr[721:1721]\n",
    "Y_train_val_3 = Y_high_snr[721:1721]\n",
    "K_mag_train_val_3 = K_mag_high_snr[721:1721]\n",
    "X_offset_train_val_3 = X_offset_high_snr[721:1721]\n",
    "\n",
    "X_train_val_4 = X_high_snr[1721:4721]\n",
    "Y_train_val_4 = Y_high_snr[1721:4721]\n",
    "K_mag_train_val_4 = K_mag_high_snr[1721:4721]\n",
    "X_offset_train_val_4 = X_offset_high_snr[1721:4721]\n",
    "\n",
    "X_train_val_5 = X_high_snr[4721:12721]\n",
    "Y_train_val_5 = Y_high_snr[4721:12721]\n",
    "K_mag_train_val_5 = K_mag_high_snr[4721:12721]\n",
    "X_offset_train_val_5 = X_offset_high_snr[4721:12721]\n",
    "\n",
    "X_train_val_6 = X_high_snr[12721:]\n",
    "Y_train_val_6 = Y_high_snr[12721:]\n",
    "K_mag_train_val_6 = K_mag_high_snr[12721:]\n",
    "X_offset_train_val_6 = X_offset_high_snr[12721:]\n",
    "\n",
    "X_train_val_7 = X_high_snr\n",
    "Y_train_val_7 = Y_high_snr\n",
    "K_mag_train_val_7 = K_mag_high_snr\n",
    "X_offset_train_val_7 = X_offset_high_snr\n",
    "\n",
    "X_test = X_low_snr\n",
    "Y_test = Y_low_snr\n",
    "K_mag_test = K_mag_low_snr\n",
    "X_offset_test = X_offset_low_snr\n",
    "snr_test = snr[low_snr_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(221, 1) (500, 1) (1000, 1) (3000, 1) (8000, 1) (15000, 1) (27721, 1) (33265, 1)\n"
     ]
    }
   ],
   "source": [
    "print(Y_train_val_1.shape , Y_train_val_2.shape , Y_train_val_3.shape, Y_train_val_4.shape , \n",
    "      Y_train_val_5.shape,Y_train_val_6.shape ,Y_train_val_7.shape , Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "pseudo-lum-input (InputLayer)   (None, 7514, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 7512, 2)      8           pseudo-lum-input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 7512, 2)      8           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 3756, 2)      0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 3754, 4)      28          max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 3754, 4)      16          conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 1877, 4)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 7508)         0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          961152      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           8256        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "offset-input (InputLayer)       (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 32)           2080        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           256         offset-input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "pseudo-lum (Dense)              (None, 1)            33          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "K_mag (InputLayer)              (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 32)           2080        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "parallax (Lambda)               (None, 1)            0           pseudo-lum[0][0]                 \n",
      "                                                                 K_mag[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "offset (Dense)                  (None, 1)            33          dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum-parallax-offset (Lambda)    (None, 1)            0           parallax[0][0]                   \n",
      "                                                                 offset[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 973,950\n",
      "Trainable params: 973,938\n",
      "Non-trainable params: 12\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_timesteps, n_features = X_train_val_1.shape[1], X_train_val_1.shape[2]\n",
    "\n",
    "Global_model = ApogeeDR14GaiaDR2(n_timesteps , n_features)\n",
    "\n",
    "Global_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22176 samples, validate on 5545 samples\n",
      "Epoch 1/200\n",
      "22176/22176 [==============================] - 19s 847us/step - loss: 6.3059 - mse: 6.3059 - val_loss: 18.5814 - val_mse: 18.5814\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 18.58138, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models//Modelo_1_D7_ReplicaBovy.h5\n",
      "Epoch 2/200\n",
      "22176/22176 [==============================] - 17s 778us/step - loss: 1.5673 - mse: 1.5673 - val_loss: 14.2141 - val_mse: 14.2141\n",
      "\n",
      "Epoch 00002: val_loss improved from 18.58138 to 14.21409, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models//Modelo_1_D7_ReplicaBovy.h5\n",
      "Epoch 3/200\n",
      "22176/22176 [==============================] - 18s 790us/step - loss: 0.8595 - mse: 0.8595 - val_loss: 6.3522 - val_mse: 6.3522\n",
      "\n",
      "Epoch 00003: val_loss improved from 14.21409 to 6.35215, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models//Modelo_1_D7_ReplicaBovy.h5\n",
      "Epoch 4/200\n",
      "22176/22176 [==============================] - 18s 819us/step - loss: 0.5842 - mse: 0.5842 - val_loss: 1.0238 - val_mse: 1.0238\n",
      "\n",
      "Epoch 00004: val_loss improved from 6.35215 to 1.02380, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models//Modelo_1_D7_ReplicaBovy.h5\n",
      "Epoch 5/200\n",
      "22176/22176 [==============================] - 18s 819us/step - loss: 0.4886 - mse: 0.4886 - val_loss: 6.6693 - val_mse: 6.6694\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.02380\n",
      "Epoch 6/200\n",
      "22176/22176 [==============================] - 18s 816us/step - loss: 0.3823 - mse: 0.3823 - val_loss: 0.6335 - val_mse: 0.6335\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.02380 to 0.63345, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models//Modelo_1_D7_ReplicaBovy.h5\n",
      "Epoch 7/200\n",
      "22176/22176 [==============================] - 18s 820us/step - loss: 0.3433 - mse: 0.3433 - val_loss: 0.6773 - val_mse: 0.6773\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.63345\n",
      "Epoch 8/200\n",
      "22176/22176 [==============================] - 18s 815us/step - loss: 0.2761 - mse: 0.2761 - val_loss: 0.5478 - val_mse: 0.5478\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.63345 to 0.54776, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models//Modelo_1_D7_ReplicaBovy.h5\n",
      "Epoch 9/200\n",
      "22176/22176 [==============================] - 18s 824us/step - loss: 0.2518 - mse: 0.2518 - val_loss: 1.3292 - val_mse: 1.3292\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.54776\n",
      "Epoch 10/200\n",
      "22176/22176 [==============================] - 18s 816us/step - loss: 0.2356 - mse: 0.2356 - val_loss: 2.5385 - val_mse: 2.5385\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.54776\n",
      "Epoch 11/200\n",
      "22176/22176 [==============================] - 18s 832us/step - loss: 0.2344 - mse: 0.2344 - val_loss: 2.9908 - val_mse: 2.9908\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.54776\n",
      "Epoch 12/200\n",
      "22176/22176 [==============================] - 18s 817us/step - loss: 0.1922 - mse: 0.1922 - val_loss: 0.4383 - val_mse: 0.4383\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.54776 to 0.43827, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models//Modelo_1_D7_ReplicaBovy.h5\n",
      "Epoch 13/200\n",
      "22176/22176 [==============================] - 18s 824us/step - loss: 0.1712 - mse: 0.1712 - val_loss: 2.0165 - val_mse: 2.0165\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.43827\n",
      "Epoch 14/200\n",
      "22176/22176 [==============================] - 18s 816us/step - loss: 0.1671 - mse: 0.1671 - val_loss: 0.8578 - val_mse: 0.8578\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.43827\n",
      "Epoch 15/200\n",
      "22176/22176 [==============================] - 18s 818us/step - loss: 0.1450 - mse: 0.1450 - val_loss: 0.8722 - val_mse: 0.8722\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.43827\n",
      "Epoch 16/200\n",
      "22176/22176 [==============================] - 18s 818us/step - loss: 0.2114 - mse: 0.2114 - val_loss: 0.7574 - val_mse: 0.7574\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.43827\n",
      "Epoch 17/200\n",
      "22176/22176 [==============================] - 18s 816us/step - loss: 0.2592 - mse: 0.2592 - val_loss: 17.5422 - val_mse: 17.5422\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.43827\n",
      "Epoch 18/200\n",
      "22176/22176 [==============================] - 18s 816us/step - loss: 0.1712 - mse: 0.1712 - val_loss: 3.2465 - val_mse: 3.2465\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.43827\n",
      "Epoch 19/200\n",
      "22176/22176 [==============================] - 18s 816us/step - loss: 0.1144 - mse: 0.1144 - val_loss: 1.4025 - val_mse: 1.4025\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.43827\n",
      "Epoch 20/200\n",
      "22176/22176 [==============================] - 18s 816us/step - loss: 0.1087 - mse: 0.1087 - val_loss: 1.0618 - val_mse: 1.0618\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.43827\n",
      "Epoch 21/200\n",
      "22176/22176 [==============================] - 18s 816us/step - loss: 0.1040 - mse: 0.1040 - val_loss: 1.8390 - val_mse: 1.8390\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.43827\n",
      "Epoch 22/200\n",
      "22176/22176 [==============================] - 18s 820us/step - loss: 0.0967 - mse: 0.0967 - val_loss: 0.3886 - val_mse: 0.3886\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.43827 to 0.38864, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models//Modelo_1_D7_ReplicaBovy.h5\n",
      "Epoch 23/200\n",
      "22176/22176 [==============================] - 18s 822us/step - loss: 0.0913 - mse: 0.0913 - val_loss: 0.5579 - val_mse: 0.5579\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.38864\n",
      "Epoch 24/200\n",
      "22176/22176 [==============================] - 18s 817us/step - loss: 0.0856 - mse: 0.0856 - val_loss: 0.4634 - val_mse: 0.4634\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.38864\n",
      "Epoch 25/200\n",
      "22176/22176 [==============================] - 18s 817us/step - loss: 0.0857 - mse: 0.0857 - val_loss: 0.4002 - val_mse: 0.4002\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.38864\n",
      "Epoch 26/200\n",
      "22176/22176 [==============================] - 18s 816us/step - loss: 0.0801 - mse: 0.0801 - val_loss: 0.7346 - val_mse: 0.7346\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.38864\n",
      "Epoch 27/200\n",
      "22176/22176 [==============================] - 18s 817us/step - loss: 0.0787 - mse: 0.0787 - val_loss: 0.3928 - val_mse: 0.3928\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.38864\n",
      "Epoch 28/200\n",
      "22176/22176 [==============================] - 19s 841us/step - loss: 0.0689 - mse: 0.0689 - val_loss: 0.3450 - val_mse: 0.3450\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.38864 to 0.34504, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models//Modelo_1_D7_ReplicaBovy.h5\n",
      "Epoch 29/200\n",
      "22176/22176 [==============================] - 18s 813us/step - loss: 0.0665 - mse: 0.0665 - val_loss: 0.4394 - val_mse: 0.4394\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.34504\n",
      "Epoch 30/200\n",
      "22176/22176 [==============================] - 18s 827us/step - loss: 0.0648 - mse: 0.0648 - val_loss: 0.3413 - val_mse: 0.3413\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.34504 to 0.34129, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models//Modelo_1_D7_ReplicaBovy.h5\n",
      "Epoch 31/200\n",
      "22176/22176 [==============================] - 18s 819us/step - loss: 0.0626 - mse: 0.0626 - val_loss: 0.3694 - val_mse: 0.3694\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.34129\n",
      "Epoch 32/200\n",
      "22176/22176 [==============================] - 18s 817us/step - loss: 0.0623 - mse: 0.0623 - val_loss: 0.3322 - val_mse: 0.3322\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.34129 to 0.33224, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models//Modelo_1_D7_ReplicaBovy.h5\n",
      "Epoch 33/200\n",
      "22176/22176 [==============================] - 18s 820us/step - loss: 0.0601 - mse: 0.0601 - val_loss: 0.4118 - val_mse: 0.4118\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.33224\n",
      "Epoch 34/200\n",
      "22176/22176 [==============================] - 18s 818us/step - loss: 0.0585 - mse: 0.0585 - val_loss: 0.3595 - val_mse: 0.3595\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.33224\n",
      "Epoch 35/200\n",
      "22176/22176 [==============================] - 19s 848us/step - loss: 0.0579 - mse: 0.0579 - val_loss: 0.3431 - val_mse: 0.3431\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.33224\n",
      "Epoch 36/200\n",
      "22176/22176 [==============================] - 19s 861us/step - loss: 0.0589 - mse: 0.0589 - val_loss: 0.4174 - val_mse: 0.4174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00036: val_loss did not improve from 0.33224\n",
      "Epoch 37/200\n",
      "22176/22176 [==============================] - 18s 808us/step - loss: 0.0601 - mse: 0.0601 - val_loss: 0.5184 - val_mse: 0.5184\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.33224\n",
      "Epoch 38/200\n",
      "22176/22176 [==============================] - 18s 806us/step - loss: 0.0505 - mse: 0.0505 - val_loss: 0.3848 - val_mse: 0.3848\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.33224\n",
      "Epoch 39/200\n",
      "22176/22176 [==============================] - 18s 804us/step - loss: 0.0490 - mse: 0.0490 - val_loss: 0.3651 - val_mse: 0.3651\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.33224\n",
      "Epoch 40/200\n",
      "22176/22176 [==============================] - 18s 805us/step - loss: 0.0484 - mse: 0.0484 - val_loss: 0.3592 - val_mse: 0.3592\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.33224\n",
      "Epoch 41/200\n",
      "22176/22176 [==============================] - 18s 808us/step - loss: 0.0477 - mse: 0.0477 - val_loss: 0.3577 - val_mse: 0.3577\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.33224\n",
      "Epoch 42/200\n",
      "22176/22176 [==============================] - 18s 820us/step - loss: 0.0463 - mse: 0.0463 - val_loss: 0.3382 - val_mse: 0.3382\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.33224\n",
      "Epoch 43/200\n",
      "22176/22176 [==============================] - 19s 845us/step - loss: 0.0444 - mse: 0.0444 - val_loss: 0.3554 - val_mse: 0.3554\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.33224\n",
      "Epoch 44/200\n",
      "22176/22176 [==============================] - 19s 862us/step - loss: 0.0439 - mse: 0.0439 - val_loss: 0.3456 - val_mse: 0.3456\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.33224\n",
      "Epoch 45/200\n",
      "22176/22176 [==============================] - 18s 817us/step - loss: 0.0436 - mse: 0.0436 - val_loss: 0.3364 - val_mse: 0.3364\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.33224\n",
      "Epoch 46/200\n",
      "22176/22176 [==============================] - 17s 776us/step - loss: 0.0432 - mse: 0.0432 - val_loss: 0.3336 - val_mse: 0.3336\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.33224\n",
      "Epoch 47/200\n",
      "22176/22176 [==============================] - 18s 833us/step - loss: 0.0427 - mse: 0.0427 - val_loss: 0.3881 - val_mse: 0.3881\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.33224\n",
      "Epoch 48/200\n",
      "22176/22176 [==============================] - 18s 822us/step - loss: 0.0416 - mse: 0.0416 - val_loss: 0.3483 - val_mse: 0.3483\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.33224\n",
      "Epoch 49/200\n",
      "22176/22176 [==============================] - 18s 815us/step - loss: 0.0411 - mse: 0.0411 - val_loss: 0.3323 - val_mse: 0.3323\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.33224\n",
      "Epoch 50/200\n",
      "22176/22176 [==============================] - 19s 849us/step - loss: 0.0410 - mse: 0.0410 - val_loss: 0.3341 - val_mse: 0.3341\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.33224\n",
      "Epoch 51/200\n",
      "22176/22176 [==============================] - 17s 774us/step - loss: 0.0407 - mse: 0.0407 - val_loss: 0.3630 - val_mse: 0.3630\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.33224\n",
      "Epoch 52/200\n",
      "22176/22176 [==============================] - 18s 815us/step - loss: 0.0408 - mse: 0.0408 - val_loss: 0.3956 - val_mse: 0.3956\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.33224\n",
      "Epoch 53/200\n",
      "22176/22176 [==============================] - 18s 824us/step - loss: 0.0399 - mse: 0.0399 - val_loss: 0.3391 - val_mse: 0.3391\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.33224\n",
      "Epoch 54/200\n",
      "22176/22176 [==============================] - 18s 819us/step - loss: 0.0398 - mse: 0.0398 - val_loss: 0.3441 - val_mse: 0.3441\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.33224\n",
      "Epoch 55/200\n",
      "22176/22176 [==============================] - 18s 800us/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.3487 - val_mse: 0.3487\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.33224\n",
      "Epoch 56/200\n",
      "22176/22176 [==============================] - 18s 794us/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.3517 - val_mse: 0.3517\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.33224\n",
      "Epoch 57/200\n",
      "22176/22176 [==============================] - 18s 819us/step - loss: 0.0394 - mse: 0.0394 - val_loss: 0.3486 - val_mse: 0.3486\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.33224\n",
      "Epoch 58/200\n",
      "22176/22176 [==============================] - 18s 796us/step - loss: 0.0391 - mse: 0.0391 - val_loss: 0.3442 - val_mse: 0.3442\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.33224\n",
      "Epoch 59/200\n",
      "22176/22176 [==============================] - 19s 834us/step - loss: 0.0390 - mse: 0.0390 - val_loss: 0.3500 - val_mse: 0.3500\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.33224\n",
      "Epoch 60/200\n",
      "22176/22176 [==============================] - 18s 812us/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.3525 - val_mse: 0.3525\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.33224\n",
      "Epoch 61/200\n",
      "22176/22176 [==============================] - 18s 807us/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.3497 - val_mse: 0.3497\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.33224\n",
      "Epoch 62/200\n",
      "22176/22176 [==============================] - 18s 830us/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.3544 - val_mse: 0.3544\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.33224\n",
      "Epoch 63/200\n",
      "22176/22176 [==============================] - 17s 780us/step - loss: 0.0386 - mse: 0.0386 - val_loss: 0.3576 - val_mse: 0.3576\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.33224\n",
      "Epoch 64/200\n",
      "22176/22176 [==============================] - 19s 858us/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.3482 - val_mse: 0.3482\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.33224\n",
      "Epoch 65/200\n",
      "22176/22176 [==============================] - 19s 859us/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.3509 - val_mse: 0.3509\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.33224\n",
      "Epoch 66/200\n",
      "22176/22176 [==============================] - 18s 806us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.3545 - val_mse: 0.3545\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.33224\n",
      "Epoch 67/200\n",
      "22176/22176 [==============================] - 19s 838us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.3532 - val_mse: 0.3532\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.33224\n",
      "Epoch 68/200\n",
      "22176/22176 [==============================] - 19s 845us/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.3525 - val_mse: 0.3525\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.33224\n",
      "Epoch 69/200\n",
      "22176/22176 [==============================] - 19s 876us/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.3518 - val_mse: 0.3518\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.33224\n",
      "Epoch 70/200\n",
      "22176/22176 [==============================] - 20s 885us/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.3523 - val_mse: 0.3523\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.33224\n",
      "Epoch 71/200\n",
      "22176/22176 [==============================] - 17s 775us/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.3545 - val_mse: 0.3545\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.33224\n",
      "Epoch 72/200\n",
      "22176/22176 [==============================] - 17s 775us/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.3512 - val_mse: 0.3512\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.33224\n",
      "Epoch 73/200\n",
      "22176/22176 [==============================] - 18s 833us/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.3537 - val_mse: 0.3537\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.33224\n",
      "Epoch 74/200\n",
      "22176/22176 [==============================] - 18s 799us/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.3511 - val_mse: 0.3511\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.33224\n",
      "Epoch 75/200\n",
      "22176/22176 [==============================] - 18s 797us/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.3540 - val_mse: 0.3540\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.33224\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22176/22176 [==============================] - 18s 833us/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.3521 - val_mse: 0.3521\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.33224\n",
      "Epoch 77/200\n",
      "22176/22176 [==============================] - 18s 819us/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.3505 - val_mse: 0.3505\n",
      "\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.33224\n",
      "Epoch 78/200\n",
      "22176/22176 [==============================] - 21s 932us/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.3522 - val_mse: 0.3522\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.33224\n",
      "Epoch 79/200\n",
      "22176/22176 [==============================] - 22s 981us/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.3525 - val_mse: 0.3525\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.33224\n",
      "Epoch 80/200\n",
      "22176/22176 [==============================] - 21s 925us/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.3518 - val_mse: 0.3518\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.33224\n",
      "Epoch 81/200\n",
      "22176/22176 [==============================] - 17s 782us/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.3516 - val_mse: 0.3516\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.33224\n",
      "Epoch 82/200\n",
      "22176/22176 [==============================] - 17s 775us/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.3544 - val_mse: 0.3544\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.33224\n",
      "Epoch 00082: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fea90190250>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Global_model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "path_local = \"/home/anell/Desktop/Bovy/AnellExercises/Good_Models/SNR100/\"\n",
    "\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=50, verbose=1, min_delta=1e-7)\n",
    "checkpoint = ModelCheckpoint(f'{path_local}/Modelo_1_D7_ReplicaBovy.h5', monitor='val_loss', \n",
    "                             verbose=1, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, verbose=1, patience=5, min_lr=0.000000001)\n",
    "\n",
    "callbacks=[reduce_lr, checkpoint, earlystopper]\n",
    "\n",
    "Global_model.fit([X_train_val_7, K_mag_train_val_7, X_offset_train_val_7], Y_train_val_7, callbacks=callbacks,\n",
    "                 epochs=200, batch_size=128, verbose=1, shuffle=\"batch\", validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### simple evaluations on test sample (SNR < 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33265/33265 [==============================] - 10s 302us/step\n"
     ]
    }
   ],
   "source": [
    "J_test , mse_test = Global_model.evaluate([X_test, K_mag_test , X_offset_test], Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350.5836360920646 350.5837097167969\n"
     ]
    }
   ],
   "source": [
    "print(J_test,mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = Global_model.predict([X_test, K_mag_test , X_offset_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(target, pred, c=\"g\", alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"target\")\n",
    "plt.ylabel(\"pred\")\n",
    "\n",
    "plt.ylim(0,100)\n",
    "plt.xlim(0,100)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
