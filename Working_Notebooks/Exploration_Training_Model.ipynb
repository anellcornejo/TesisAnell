{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping , ReduceLROnPlateau , ModelCheckpoint\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Conv1D , Dropout , Flatten , MaxPooling1D, Dense, Input\n",
    "from keras.layers.core import Lambda\n",
    "from keras.models import Model\n",
    "import pandas as pd\n",
    "import random\n",
    "import h5py\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descarga y orden de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos\n",
    "with h5py.File('/home/anell/Desktop/Bovy/AnellExercises/Fits_files/apogeedr14_gaiadr2_with_spectrum.h5') as F:  # ensure the file will be cleaned up\n",
    "    parallax = np.array(F['parallax'])\n",
    "    parallax_error = np.array(F['parallax_err'])\n",
    "    spectra = np.array(F['spectra'])\n",
    "    Kcorr = np.array(F['corrected_magnitude_K'])  # extinction corrected Ks\n",
    "    bp_rp = np.array(F['bp_rp'])\n",
    "    phot_g_mean_mag = np.array(F['phot_g_mean_mag'])\n",
    "    teff = np.array(F['NN_teff'])\n",
    "    apogee_id = np.array(F['APOGEE_ID'])\n",
    "    snr = np.array(F['SNR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bp_rp), parallax.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizamos \n",
    "phot_g_mean_mag_std = np.std(phot_g_mean_mag)\n",
    "phot_g_mean_mag_mean = np.mean(phot_g_mean_mag)\n",
    "norm_phot_g_mean_mag = (phot_g_mean_mag - phot_g_mean_mag_mean) / phot_g_mean_mag_std\n",
    "\n",
    "bp_rp_std = np.std(bp_rp)\n",
    "bp_rp_mean = np.mean(bp_rp)\n",
    "norm_bp_rp = (bp_rp - bp_rp_mean) / bp_rp_std\n",
    "\n",
    "teff_std = np.std(teff)\n",
    "teff_mean = np.mean(teff)\n",
    "norm_teff = (teff - teff_mean) / teff_std\n",
    "\n",
    "\n",
    "#EStablecemos las variables que entrarán a la red y corregimos sus dimensiones\n",
    "X = np.expand_dims(spectra,axis = 2)\n",
    "Y = np.expand_dims(parallax,axis=1)\n",
    "Y_error = np.expand_dims(parallax_error,axis=1)\n",
    "K_mag = np.expand_dims(Kcorr,axis=1)\n",
    "G_mag = np.expand_dims(norm_phot_g_mean_mag,axis=1)\n",
    "Bp_Rp = np.expand_dims(norm_bp_rp,axis=1)\n",
    "Teff = np.expand_dims(norm_teff,axis=1)\n",
    "Snr = np.expand_dims(snr,axis=1)\n",
    "\n",
    "X_offset = np.concatenate((G_mag, Bp_Rp , Teff), axis = 1) \n",
    "\n",
    "\n",
    "#Aleatorizamos la muestra\n",
    "idx = []\n",
    "for i in range(len(X)):\n",
    "    idx.append(i)\n",
    "random.seed(20)\n",
    "random.shuffle(idx)\n",
    "\n",
    "X = X[idx]                  # shape: (60986, 7514 , 1)   \n",
    "Y = Y[idx]                  # shape: (60986, 1)  \n",
    "K_mag = K_mag[idx]          # shape: (60986, 1) \n",
    "X_offset = X_offset[idx]    # shape: (60986, 3)\n",
    "SNR = Snr[idx]              # shape: (60986, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y.shape, K_mag.shape, X_offset.shape , SNR.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corte por buena o mala relación señal a ruido\n",
    "idx_snr_good = []\n",
    "idx_snr_bad = []\n",
    "for i in range(len(SNR)):\n",
    "    if snr[i] >= 200:\n",
    "        idx_snr_good.append(i)\n",
    "    else:\n",
    "        idx_snr_bad.append(i)\n",
    "\n",
    "len(idx_snr_good), len(idx_snr_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_dataset(X , Y , K_mag , X_offset , idx_snr_good, idx_snr_bad, m = None ):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    X.shape = (_ , 7514 , 1)\n",
    "    Y.shape = (_ , 1)\n",
    "    K_mag.shape = (_ , 1)\n",
    "    X_offset.shape = (_ , 3)\n",
    "    \"\"\"\n",
    "    if m == None:\n",
    "        m = len(Y)\n",
    "    \n",
    "    X_train = np.concatenate((X[idx_snr_good][:int(0.4*m)],X[idx_snr_bad][:int(0.2*m)]),axis = 0)\n",
    "    Y_train = np.concatenate((Y[idx_snr_good][:int(0.4*m)],Y[idx_snr_bad][:int(0.2*m)]),axis = 0)\n",
    "    K_mag_train = np.concatenate((K_mag[idx_snr_good][:int(0.4*m)],K_mag[idx_snr_bad][:int(0.2*m)]),axis = 0)\n",
    "    X_offset_train = np.concatenate((X_offset[idx_snr_good][:int(0.4*m)],X_offset[idx_snr_good][:int(0.2*m)]),axis = 0)\n",
    "        \n",
    "    X_val = np.concatenate((X[idx_snr_good][int(0.4*m):int(0.6*m)],X[idx_snr_bad][int(0.4*m):int(0.5*m)]),axis = 0)\n",
    "    Y_val = np.concatenate((Y[idx_snr_good][int(0.4*m):int(0.6*m)],Y[idx_snr_bad][int(0.4*m):int(0.5*m)]),axis = 0)\n",
    "    K_mag_val = np.concatenate((K_mag[idx_snr_good][int(0.4*m):int(0.6*m)],K_mag[idx_snr_bad][int(0.4*m):int(0.5*m)]),axis = 0)\n",
    "    X_offset_val = np.concatenate((X_offset[idx_snr_good][int(0.4*m):int(0.6*m)],X_offset[idx_snr_bad][int(0.4*m):int(0.5*m)]),axis = 0)\n",
    "        \n",
    "    X_test = X[idx_snr_bad][int(0.5*m):int(m)]\n",
    "    Y_test = Y[idx_snr_bad][int(0.5*m):int(m)]\n",
    "    K_mag_test = K_mag[idx_snr_bad][int(0.5*m):int(m)]\n",
    "    X_offset_test = X_offset[idx_snr_bad][int(0.5*m):int(m)]\n",
    "    \n",
    "    #Aleatorizamos las variables:\n",
    "    idx_train = []\n",
    "    for i in range(len(X_train)):\n",
    "        idx_train.append(i)\n",
    "    random.seed(50)\n",
    "    random.shuffle(idx_train)\n",
    "    \n",
    "    idx_val = []\n",
    "    for j in range(len(X_val)):\n",
    "        idx_val.append(j)\n",
    "    random.seed(100)\n",
    "    random.shuffle(idx_val) \n",
    "    \n",
    "    X_train = X_train[idx_train]\n",
    "    Y_train = Y_train[idx_train]\n",
    "    K_mag_train = K_mag_train[idx_train]\n",
    "    X_offset_train = X_offset_train[idx_train]\n",
    "        \n",
    "    X_val = X_val[idx_val]\n",
    "    Y_val = Y_val[idx_val]\n",
    "    K_mag_val = K_mag_val[idx_val]\n",
    "    X_offset_val = X_offset_val[idx_val]\n",
    "    \n",
    "\n",
    "    return ([X_train, Y_train, K_mag_train, X_offset_train], [X_val, Y_val, K_mag_val, X_offset_val],\n",
    "            [X_test, Y_test, K_mag_test, X_offset_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establecemos los sets de entrenaiento, de validación y de testeo\n",
    "TRAIN,VAL,TEST = size_dataset(X , Y , K_mag , X_offset , idx_snr_good, idx_snr_bad, m = 10000 )\n",
    "\n",
    "X_train, Y_train, K_mag_train, X_offset_train = TRAIN\n",
    "X_val, Y_val, K_mag_val, X_offset_val = VAL\n",
    "X_test, Y_test, K_mag_test, X_offset_test = TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape , X_val.shape , X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generación del modelo y entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ApogeeDR14GaiaDR2(dim_t , dim_n): \n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    dim_t - number of time steps of spectrum \n",
    "    dim_n - number of features of spectrum\n",
    "    \"\"\"\n",
    "    \n",
    "    #SPECTRUM TO LUINOSITY\n",
    "    dim_1 = 1 # number of corrected magnitude for one example \n",
    "    units = 1 #number of final output for one example\n",
    "    inputs_spectra = Input(shape=(dim_t, dim_n)) \n",
    "    inputs_mag = Input(shape=(dim_1,), name=\"ApparentMagnitude-input\")\n",
    "    \n",
    "    x_parallax = Conv1D(filters=4, kernel_size=3, activation='relu')(inputs_spectra)\n",
    "    x_parallax = MaxPooling1D(pool_size=2)(x_parallax)\n",
    "    x_parallax = Flatten()(x_parallax)\n",
    "    #x_parallax = Dense(164, activation='relu')(x_parallax) \n",
    "    x_parallax = Dense(64, activation='tanh')(x_parallax) \n",
    "    x_parallax = Dense(32, activation='tanh')(x_parallax)\n",
    "    x_parallax = Dense(16, activation='tanh')(x_parallax)\n",
    "    x_parallax = Dense(units, activation='softplus')(x_parallax)\n",
    "    \n",
    "    outputs_parallax = Lambda(lambda function: tf.math.multiply(function[0], tf.math.pow(10., \n",
    "                              tf.math.multiply(-0.2, function[1]))),\n",
    "                              name='luminosity-to-parallax')([x_parallax, inputs_mag])\n",
    "   \n",
    "    #OFFSET CORRECTION : (optimization)\n",
    "    inputs_offset = Input(shape=(3,), name=\"Offset-input\")\n",
    "    x_offset = Dense(64, activation='relu')(inputs_offset)\n",
    "    x_offset = Dense(32, activation='relu')(inputs_offset) \n",
    "    x_offset = Dense(16, activation='relu')(x_offset)\n",
    "    x_offset = Dense(units, activation='tanh')(x_offset) \n",
    "    \n",
    "    outputs_parallax_with_offset = Lambda(lambda function: tf.math.add(function[0], function[1]),\n",
    "                                          name=\"Sum-parallax-offset\")([outputs_parallax, x_offset]) \n",
    "    \n",
    "    #Model setup\n",
    "    model =  Model(inputs = [inputs_spectra,inputs_mag, inputs_offset],outputs = [outputs_parallax_with_offset])\n",
    "    \n",
    "    return model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps, n_features = X_train.shape[1], X_train.shape[2]\n",
    "\n",
    "Global_model = ApogeeDR14GaiaDR2(n_timesteps , n_features)\n",
    "\n",
    "Global_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "24/24 [==============================] - ETA: 0s - loss: 19.9126 - mse: 19.9126\n",
      "Epoch 00001: val_loss improved from inf to 18.44538, saving model to /home/anell/Desktop/TesisAnell/Models_NN/model_1.h5\n",
      "24/24 [==============================] - 3s 106ms/step - loss: 19.9126 - mse: 19.9126 - val_loss: 18.4454 - val_mse: 18.4454\n",
      "Epoch 2/30\n",
      "24/24 [==============================] - ETA: 0s - loss: 18.5397 - mse: 18.5397\n",
      "Epoch 00002: val_loss improved from 18.44538 to 17.63850, saving model to /home/anell/Desktop/TesisAnell/Models_NN/model_1.h5\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 18.5397 - mse: 18.5397 - val_loss: 17.6385 - val_mse: 17.6385\n",
      "Epoch 3/30\n",
      "24/24 [==============================] - ETA: 0s - loss: 17.7065 - mse: 17.7065\n",
      "Epoch 00003: val_loss improved from 17.63850 to 17.37197, saving model to /home/anell/Desktop/TesisAnell/Models_NN/model_1.h5\n",
      "24/24 [==============================] - 2s 97ms/step - loss: 17.7065 - mse: 17.7065 - val_loss: 17.3720 - val_mse: 17.3720\n",
      "Epoch 4/30\n",
      "24/24 [==============================] - ETA: 0s - loss: 17.3061 - mse: 17.3061\n",
      "Epoch 00004: val_loss did not improve from 17.37197\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 17.3061 - mse: 17.3061 - val_loss: 17.3756 - val_mse: 17.3756\n",
      "Epoch 5/30\n",
      "24/24 [==============================] - ETA: 0s - loss: 17.1168 - mse: 17.1168\n",
      "Epoch 00005: val_loss did not improve from 17.37197\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 17.1168 - mse: 17.1168 - val_loss: 17.5417 - val_mse: 17.5417\n",
      "Epoch 6/30\n",
      "24/24 [==============================] - ETA: 0s - loss: 16.9906 - mse: 16.9906\n",
      "Epoch 00006: val_loss did not improve from 17.37197\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 16.9906 - mse: 16.9906 - val_loss: 17.8268 - val_mse: 17.8268\n",
      "Epoch 7/30\n",
      "24/24 [==============================] - ETA: 0s - loss: 16.8777 - mse: 16.8777\n",
      "Epoch 00007: val_loss did not improve from 17.37197\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 16.8777 - mse: 16.8777 - val_loss: 18.2138 - val_mse: 18.2138\n",
      "Epoch 8/30\n",
      "24/24 [==============================] - ETA: 0s - loss: 16.8234 - mse: 16.8234\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 17.37197\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 16.8234 - mse: 16.8234 - val_loss: 18.7235 - val_mse: 18.7235\n",
      "Epoch 9/30\n",
      "24/24 [==============================] - ETA: 0s - loss: 16.7090 - mse: 16.7090\n",
      "Epoch 00009: val_loss did not improve from 17.37197\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 16.7090 - mse: 16.7090 - val_loss: 18.7790 - val_mse: 18.7790\n",
      "Epoch 10/30\n",
      "24/24 [==============================] - ETA: 0s - loss: 16.7164 - mse: 16.7164\n",
      "Epoch 00010: val_loss did not improve from 17.37197\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 16.7164 - mse: 16.7164 - val_loss: 18.8361 - val_mse: 18.8361\n",
      "Epoch 11/30\n",
      "24/24 [==============================] - ETA: 0s - loss: 16.7491 - mse: 16.7491\n",
      "Epoch 00011: val_loss did not improve from 17.37197\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 16.7491 - mse: 16.7491 - val_loss: 18.8882 - val_mse: 18.8882\n",
      "Epoch 12/30\n",
      "24/24 [==============================] - ETA: 0s - loss: 16.7201 - mse: 16.7201\n",
      "Epoch 00012: val_loss did not improve from 17.37197\n",
      "24/24 [==============================] - 2s 96ms/step - loss: 16.7201 - mse: 16.7201 - val_loss: 18.9453 - val_mse: 18.9453\n",
      "Epoch 13/30\n",
      "24/24 [==============================] - ETA: 0s - loss: 16.7337 - mse: 16.7337\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 17.37197\n",
      "24/24 [==============================] - 2s 101ms/step - loss: 16.7337 - mse: 16.7337 - val_loss: 18.9963 - val_mse: 18.9963\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f63c45e7250>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Global_model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "path_local = \"/home/anell/Desktop/TesisAnell/Models_NN\"\n",
    "\n",
    "callbacks = [EarlyStopping(patience=10, verbose=1), ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1), \n",
    "             ModelCheckpoint(f'{path_local}/model_1.h5', verbose=1, save_best_only=True)]\n",
    "\n",
    "Global_model.fit([X_train,K_mag_train,X_offset_train], Y_train, batch_size=256, shuffle='batch', \n",
    "                 epochs=30, callbacks=callbacks, validation_data=([X_val,K_mag_val,X_offset_val], Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
