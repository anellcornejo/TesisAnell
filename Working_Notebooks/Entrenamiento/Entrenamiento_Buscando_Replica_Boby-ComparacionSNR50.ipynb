{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.layers import Conv1D , Dropout , Flatten , MaxPooling1D, Dense, Input, BatchNormalization\n",
    "from keras.layers.core import Lambda\n",
    "from keras.models import Model , load_model\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import h5py\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ApogeeDR14GaiaDR2(dim_t , dim_n): \n",
    "    \"\"\"\n",
    "    INPUT: \n",
    "    dim_t - number of time steps of spectrum \n",
    "    dim_n - number of features of spectrum\n",
    "    \"\"\"\n",
    "    \n",
    "    #SPECTRUM TO LUINOSITY\n",
    "    dim_1 = 1 # number of corrected magnitude for one example \n",
    "    units = 1 #number of final output for one example+\n",
    "    \n",
    "    inputs_spectra = Input(shape=(dim_t, dim_n), name=\"pseudo-lum-input\") \n",
    "    inputs_mag = Input(shape=(dim_1,), name=\"K_mag\")\n",
    "    \n",
    "    x_parallax = Conv1D(filters=2, kernel_size=3, activation='relu')(inputs_spectra)\n",
    "    x_parallax = BatchNormalization()(x_parallax)\n",
    "    x_parallax = MaxPooling1D(pool_size=2)(x_parallax)\n",
    "    \n",
    "    x_parallax = Conv1D(filters=4, kernel_size=3, activation='relu')(x_parallax)\n",
    "    x_parallax = BatchNormalization()(x_parallax)\n",
    "    x_parallax = MaxPooling1D(pool_size=2)(x_parallax)\n",
    "    \n",
    "    x_parallax = Flatten()(x_parallax)\n",
    "    x_parallax = Dense(128, activation='relu')(x_parallax) \n",
    "    x_parallax = Dense(64, activation='relu')(x_parallax) \n",
    "    x_parallax = Dense(32, activation='relu')(x_parallax)\n",
    "    x_parallax = Dense(units, activation='softplus', name=\"pseudo-lum\")(x_parallax) \n",
    "      \n",
    "    #OFFSET CORRECTION : (optimization)\n",
    "    inputs_offset = Input(shape=(3,), name=\"offset-input\")\n",
    "    x_offset = Dense(64, activation='relu')(inputs_offset)\n",
    "    x_offset = Dense(32, activation='relu')(x_offset) \n",
    "    x_offset = Dense(units, activation='tanh', name=\"offset\")(x_offset) \n",
    "    \n",
    "    #Functions\n",
    "    outputs_parallax = Lambda(lambda function: tf.math.multiply(function[0], tf.math.pow(10., \n",
    "                              tf.math.multiply(-0.2, function[1]))),\n",
    "                              name='parallax')([x_parallax, inputs_mag])\n",
    "    \n",
    "    outputs_parallax_with_offset = Lambda(lambda function: tf.math.add(function[0], function[1]),\n",
    "                                          name=\"sum-parallax-offset\")([outputs_parallax, x_offset]) \n",
    "    \n",
    "    #Model setup\n",
    "    model =  Model(inputs = [inputs_spectra,inputs_mag, inputs_offset],outputs = [outputs_parallax_with_offset])\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos\n",
    "path_local_data = '/home/anell/Desktop/Bovy/AnellExercises/Fits_files'\n",
    "#path_local_data = '/home/bapanes/Research-Now/local/astronn-local/apo-gaia/'\n",
    "\n",
    "with h5py.File(f'{path_local_data}/apogeedr14_gaiadr2_with_spectrum_probando_rendimiento_2.h5','r') as F:  \n",
    "    parallax = np.array(F['parallax'])\n",
    "    parallax_error = np.array(F['parallax_err'])\n",
    "    spectra = np.array(F['spectra'])\n",
    "    Kmag = np.array(F['corrected_magnitude_K'])\n",
    "    bp_rp = np.array(F['bp_rp'])\n",
    "    Gmag = np.array(F['phot_g_mean_mag'])\n",
    "    teff = np.array(F['NN_teff'])\n",
    "    apogee_id = np.array(F['APOGEE_ID'])\n",
    "    snr = np.array(F['SNR'])\n",
    "    fe_h = np.array(F['Fe/H'])\n",
    "    path_spectra = np.array(F['Path_spectra'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60986,), (60986, 7514), (60986,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallax.shape , spectra.shape , Kmag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establecemos las variables que entrarán a la red y corregimos sus dimensiones\n",
    "X = np.expand_dims(spectra, axis = 2)\n",
    "Y = np.expand_dims(parallax, axis = 1)\n",
    "K_mag = np.expand_dims(Kmag, axis = 1)\n",
    "\n",
    "# Normalizamos Gmag , el color (G_bp - G_rp) y teff\n",
    "Gmag_std = np.std(Gmag)\n",
    "Gmag_mean = np.mean(Gmag)\n",
    "Gmag_norm = (Gmag - Gmag_mean) / Gmag_std\n",
    "\n",
    "bp_rp_std = np.std(bp_rp)\n",
    "bp_rp_mean = np.mean(bp_rp)\n",
    "bp_rp_norm = (bp_rp - bp_rp_mean) / bp_rp_std\n",
    "\n",
    "teff_std = np.std(teff)\n",
    "teff_mean = np.mean(teff)\n",
    "teff_norm = (teff - teff_mean) / teff_std\n",
    "\n",
    "G_mag = np.expand_dims(Gmag_norm, axis=1)\n",
    "Bp_Rp = np.expand_dims(bp_rp_norm, axis=1)\n",
    "Teff = np.expand_dims(teff_norm, axis=1)\n",
    "\n",
    "X_offset = np.concatenate((G_mag, Bp_Rp , Teff), axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60986, 7514, 1) (60986, 1) (60986, 1) (60986, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape , Y.shape, K_mag.shape, X_offset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNR cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_snr_idx = []\n",
    "low_snr_idx = []\n",
    "\n",
    "for i in range(len(snr)):\n",
    "    if snr[i] >= 50:           \n",
    "        high_snr_idx.append(i)\n",
    "    else:\n",
    "        low_snr_idx.append(i)\n",
    "\n",
    "random.seed(10)\n",
    "random.shuffle(high_snr_idx)\n",
    "random.seed(60)\n",
    "random.shuffle(low_snr_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR>50: 59857, else: 1129\n"
     ]
    }
   ],
   "source": [
    "print(\"SNR>50: %d, else: %d\"%(len(high_snr_idx), len(low_snr_idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diseño Experimental:\n",
    "\n",
    "Consideraré train (0.8) y valid (0.2)\n",
    "\n",
    "train_val_1 $\\rightarrow$  177 (train) + 44 (valid) = 357\n",
    "\n",
    "train_val_2 $\\rightarrow$  400 (train) + 100 (valid) = 500\n",
    "\n",
    "train_val_3 $\\rightarrow$  800 (train) + 200 (valid) = 1000\n",
    "\n",
    "train_val_4 $\\rightarrow$  2400 (train) + 600 (valid) = 8000\n",
    "\n",
    "train_val_5 $\\rightarrow$  6400 (train) + 1600 (valid) = 15000 \n",
    "\n",
    "train_val_6 $\\rightarrow$  12000 (train) + 3000 (valid) = 35000 \n",
    "\n",
    "train_val_7 $\\rightarrow$  22177 (train) + 5594 (valid) = 59857\n",
    "\n",
    "test $\\rightarrow$ 1129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_high_snr = X[high_snr_idx]\n",
    "Y_high_snr = Y[high_snr_idx]\n",
    "K_mag_high_snr = K_mag[high_snr_idx]\n",
    "X_offset_high_snr = X_offset[high_snr_idx]\n",
    "\n",
    "X_low_snr = X[low_snr_idx]\n",
    "Y_low_snr = Y[low_snr_idx]\n",
    "K_mag_low_snr = K_mag[low_snr_idx]\n",
    "X_offset_low_snr = X_offset[low_snr_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val_1 = X_high_snr[:357]\n",
    "Y_train_val_1 = Y_high_snr[:357]\n",
    "K_mag_train_val_1 = K_mag_high_snr[:357]\n",
    "X_offset_train_val_1 = X_offset_high_snr[:357]\n",
    "\n",
    "X_train_val_2 = X_high_snr[357:857]\n",
    "Y_train_val_2 = Y_high_snr[357:857]\n",
    "K_mag_train_val_2 = K_mag_high_snr[357:857]\n",
    "X_offset_train_val_2 = X_offset_high_snr[357:857]\n",
    "\n",
    "X_train_val_3 = X_high_snr[857:1857]\n",
    "Y_train_val_3 = Y_high_snr[857:1857]\n",
    "K_mag_train_val_3 = K_mag_high_snr[857:1857]\n",
    "X_offset_train_val_3 = X_offset_high_snr[857:1857]\n",
    "\n",
    "X_train_val_4 = X_high_snr[1857:9857]\n",
    "Y_train_val_4 = Y_high_snr[1857:9857]\n",
    "K_mag_train_val_4 = K_mag_high_snr[1857:9857]\n",
    "X_offset_train_val_4 = X_offset_high_snr[1857:9857]\n",
    "\n",
    "X_train_val_5 = X_high_snr[9857:24857]\n",
    "Y_train_val_5 = Y_high_snr[9857:24857]\n",
    "K_mag_train_val_5 = K_mag_high_snr[9857:24857]\n",
    "X_offset_train_val_5 = X_offset_high_snr[9857:24857]\n",
    "\n",
    "X_train_val_6 = X_high_snr[24857:]\n",
    "Y_train_val_6 = Y_high_snr[24857:]\n",
    "K_mag_train_val_6 = K_mag_high_snr[24857:]\n",
    "X_offset_train_val_6 = X_offset_high_snr[24857:]\n",
    "\n",
    "X_train_val_7 = X_high_snr\n",
    "Y_train_val_7 = Y_high_snr\n",
    "K_mag_train_val_7 = K_mag_high_snr\n",
    "X_offset_train_val_7 = X_offset_high_snr\n",
    "\n",
    "X_test = X_low_snr\n",
    "Y_test = Y_low_snr\n",
    "K_mag_test = K_mag_low_snr\n",
    "X_offset_test = X_offset_low_snr\n",
    "snr_test = snr[low_snr_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(357, 1) (500, 1) (1000, 1) (8000, 1) (15000, 1) (35000, 1) (59857, 1) (1129, 1)\n"
     ]
    }
   ],
   "source": [
    "print(Y_train_val_1.shape , Y_train_val_2.shape , Y_train_val_3.shape, Y_train_val_4.shape , \n",
    "      Y_train_val_5.shape,Y_train_val_6.shape ,Y_train_val_7.shape , Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "pseudo-lum-input (InputLayer)   (None, 7514, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 7512, 2)      8           pseudo-lum-input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 7512, 2)      8           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 3756, 2)      0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 3754, 4)      28          max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 3754, 4)      16          conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 1877, 4)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 7508)         0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          961152      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           8256        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "offset-input (InputLayer)       (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 32)           2080        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           256         offset-input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "pseudo-lum (Dense)              (None, 1)            33          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "K_mag (InputLayer)              (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 32)           2080        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "parallax (Lambda)               (None, 1)            0           pseudo-lum[0][0]                 \n",
      "                                                                 K_mag[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "offset (Dense)                  (None, 1)            33          dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum-parallax-offset (Lambda)    (None, 1)            0           parallax[0][0]                   \n",
      "                                                                 offset[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 973,950\n",
      "Trainable params: 973,938\n",
      "Non-trainable params: 12\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_timesteps, n_features = X_train_val_5.shape[1], X_train_val_5.shape[2]\n",
    "\n",
    "Global_model = ApogeeDR14GaiaDR2(n_timesteps , n_features)\n",
    "\n",
    "Global_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 3000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 11s 952us/step - loss: 813.7715 - mse: 813.7715 - val_loss: 14.0046 - val_mse: 14.0046\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 14.00464, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/SNR50//Modelo_1_D5_ReplicaBovy_SNR50_prueba.h5\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 9s 779us/step - loss: 35.8294 - mse: 35.8294 - val_loss: 13.8551 - val_mse: 13.8551\n",
      "\n",
      "Epoch 00002: val_loss improved from 14.00464 to 13.85511, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/SNR50//Modelo_1_D5_ReplicaBovy_SNR50_prueba.h5\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 10s 803us/step - loss: 24.8177 - mse: 24.8177 - val_loss: 13.7565 - val_mse: 13.7565\n",
      "\n",
      "Epoch 00003: val_loss improved from 13.85511 to 13.75646, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/SNR50//Modelo_1_D5_ReplicaBovy_SNR50_prueba.h5\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 10s 816us/step - loss: 96.0704 - mse: 96.0704 - val_loss: 16.3009 - val_mse: 16.3009\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 13.75646\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 10s 814us/step - loss: 363.0086 - mse: 363.0087 - val_loss: 13.0349 - val_mse: 13.0349\n",
      "\n",
      "Epoch 00005: val_loss improved from 13.75646 to 13.03491, saving model to /home/anell/Desktop/Bovy/AnellExercises/Good_Models/SNR50//Modelo_1_D5_ReplicaBovy_SNR50_prueba.h5\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 10s 827us/step - loss: 146.9533 - mse: 146.9533 - val_loss: 14.2962 - val_mse: 14.2962\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 13.03491\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 10s 830us/step - loss: 1.9909 - mse: 1.9909 - val_loss: 73.2563 - val_mse: 73.2563\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 13.03491\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 10s 829us/step - loss: 1.2598 - mse: 1.2598 - val_loss: 243.2098 - val_mse: 243.2098\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 13.03491\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 10s 831us/step - loss: 1.0320 - mse: 1.0320 - val_loss: 386.5171 - val_mse: 386.5170\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 13.03491\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 10s 858us/step - loss: 1.0635 - mse: 1.0635 - val_loss: 318.3178 - val_mse: 318.3178\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 13.03491\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 10s 832us/step - loss: 1.0212 - mse: 1.0212 - val_loss: 383.0891 - val_mse: 383.0892\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 13.03491\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 11s 937us/step - loss: 0.9211 - mse: 0.9211 - val_loss: 456.0835 - val_mse: 456.0835\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 13.03491\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 9s 777us/step - loss: 0.8077 - mse: 0.8077 - val_loss: 505.8127 - val_mse: 505.8127\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 13.03491\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 10s 794us/step - loss: 0.7261 - mse: 0.7261 - val_loss: 514.7937 - val_mse: 514.7937\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 13.03491\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 10s 865us/step - loss: 0.6753 - mse: 0.6753 - val_loss: 575.0189 - val_mse: 575.0189\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 13.03491\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 9s 778us/step - loss: 0.6357 - mse: 0.6357 - val_loss: 584.5260 - val_mse: 584.5260\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 13.03491\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 10s 809us/step - loss: 0.6136 - mse: 0.6136 - val_loss: 577.9963 - val_mse: 577.9964\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 13.03491\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 9s 781us/step - loss: 0.5947 - mse: 0.5947 - val_loss: 580.7743 - val_mse: 580.7744\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 13.03491\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 9s 775us/step - loss: 0.5788 - mse: 0.5788 - val_loss: 582.5722 - val_mse: 582.5723\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 13.03491\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 9s 774us/step - loss: 0.5599 - mse: 0.5599 - val_loss: 598.7026 - val_mse: 598.7025\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 13.03491\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 9s 791us/step - loss: 0.5437 - mse: 0.5437 - val_loss: 593.9742 - val_mse: 593.9741\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 13.03491\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 9s 783us/step - loss: 0.5354 - mse: 0.5354 - val_loss: 591.2012 - val_mse: 591.2012\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 13.03491\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 9s 790us/step - loss: 0.5278 - mse: 0.5278 - val_loss: 582.4914 - val_mse: 582.4914\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 13.03491\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 10s 827us/step - loss: 0.5209 - mse: 0.5209 - val_loss: 584.9733 - val_mse: 584.9733\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 13.03491\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 11s 888us/step - loss: 0.5147 - mse: 0.5147 - val_loss: 590.8391 - val_mse: 590.8391\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 13.03491\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 10s 811us/step - loss: 0.5062 - mse: 0.5062 - val_loss: 588.3043 - val_mse: 588.3044\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 13.03491\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 11s 902us/step - loss: 0.5027 - mse: 0.5027 - val_loss: 588.3181 - val_mse: 588.3181\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 13.03491\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 10s 834us/step - loss: 0.4993 - mse: 0.4993 - val_loss: 584.2415 - val_mse: 584.2415\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 13.03491\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 10s 806us/step - loss: 0.4962 - mse: 0.4962 - val_loss: 582.4626 - val_mse: 582.4626\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 13.03491\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 9s 781us/step - loss: 0.4927 - mse: 0.4927 - val_loss: 587.1396 - val_mse: 587.1398\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 13.03491\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 9s 781us/step - loss: 0.4888 - mse: 0.4888 - val_loss: 589.2965 - val_mse: 589.2966\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 13.03491\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 11s 924us/step - loss: 0.4872 - mse: 0.4872 - val_loss: 586.6653 - val_mse: 586.6652\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 13.03491\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 10s 864us/step - loss: 0.4852 - mse: 0.4852 - val_loss: 582.0967 - val_mse: 582.0966\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 13.03491\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 11s 883us/step - loss: 0.4838 - mse: 0.4838 - val_loss: 576.3912 - val_mse: 576.3912\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 13.03491\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 10s 851us/step - loss: 0.4824 - mse: 0.4824 - val_loss: 580.9178 - val_mse: 580.9177\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 13.03491\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 10s 799us/step - loss: 0.4800 - mse: 0.4800 - val_loss: 585.1691 - val_mse: 585.1691\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 13.03491\n",
      "Epoch 37/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 9s 774us/step - loss: 0.4791 - mse: 0.4791 - val_loss: 586.1676 - val_mse: 586.1677\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 13.03491\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 10s 795us/step - loss: 0.4784 - mse: 0.4784 - val_loss: 585.2159 - val_mse: 585.2159\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 13.03491\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 9s 774us/step - loss: 0.4775 - mse: 0.4775 - val_loss: 589.8594 - val_mse: 589.8594\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 13.03491\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 10s 812us/step - loss: 0.4766 - mse: 0.4766 - val_loss: 589.0441 - val_mse: 589.0441\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 13.03491\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 10s 840us/step - loss: 0.4756 - mse: 0.4756 - val_loss: 584.6866 - val_mse: 584.6866\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 13.03491\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 10s 814us/step - loss: 0.4753 - mse: 0.4753 - val_loss: 585.3768 - val_mse: 585.3768\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 13.03491\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 10s 793us/step - loss: 0.4746 - mse: 0.4746 - val_loss: 578.9597 - val_mse: 578.9597\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 13.03491\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 10s 813us/step - loss: 0.4742 - mse: 0.4742 - val_loss: 583.1923 - val_mse: 583.1924\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 13.03491\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 9s 772us/step - loss: 0.4740 - mse: 0.4740 - val_loss: 588.5185 - val_mse: 588.5185\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 13.03491\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 9s 791us/step - loss: 0.4732 - mse: 0.4732 - val_loss: 590.3874 - val_mse: 590.3875\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 13.03491\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 11s 884us/step - loss: 0.4730 - mse: 0.4730 - val_loss: 589.8312 - val_mse: 589.8312\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 13.03491\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 11s 955us/step - loss: 0.4728 - mse: 0.4728 - val_loss: 580.8518 - val_mse: 580.8518\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 13.03491\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 11s 905us/step - loss: 0.4725 - mse: 0.4725 - val_loss: 584.0285 - val_mse: 584.0285\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 13.03491\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 11s 892us/step - loss: 0.4722 - mse: 0.4722 - val_loss: 579.5121 - val_mse: 579.5121\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 13.03491\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 10s 848us/step - loss: 0.4720 - mse: 0.4720 - val_loss: 577.1112 - val_mse: 577.1113\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 13.03491\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 11s 920us/step - loss: 0.4718 - mse: 0.4718 - val_loss: 576.8241 - val_mse: 576.8240\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 13.03491\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 10s 817us/step - loss: 0.4717 - mse: 0.4717 - val_loss: 581.6068 - val_mse: 581.6068\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 13.03491\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 11s 919us/step - loss: 0.4716 - mse: 0.4716 - val_loss: 587.0775 - val_mse: 587.0775\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 13.03491\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 11s 887us/step - loss: 0.4715 - mse: 0.4715 - val_loss: 583.3185 - val_mse: 583.3184\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 13.03491\n",
      "Epoch 00055: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f522c5bad10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Global_model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "path_local = \"/home/anell/Desktop/Bovy/AnellExercises/Good_Models/SNR50/\"\n",
    "\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=50, verbose=1, min_delta=1e-7)\n",
    "checkpoint = ModelCheckpoint(f'{path_local}/Modelo_1_D5_ReplicaBovy_SNR50.h5', monitor='val_loss', \n",
    "                             verbose=1, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, verbose=1, patience=5, min_lr=0.000000001)\n",
    "\n",
    "callbacks=[reduce_lr, checkpoint, earlystopper]\n",
    "\n",
    "Global_model.fit([X_train_val_5, K_mag_train_val_5, X_offset_train_val_5], Y_train_val_5, callbacks=callbacks,\n",
    "                 epochs=200, batch_size=128, verbose=1, shuffle=\"batch\", validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### simple evaluations on test sample (SNR < 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1129/1129 [==============================] - 0s 342us/step\n"
     ]
    }
   ],
   "source": [
    "J_test , mse_test = Global_model.evaluate([X_test, K_mag_test , X_offset_test], Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.91263245417018 4.912633419036865\n"
     ]
    }
   ],
   "source": [
    "print(J_test,mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = Global_model.predict([X_test, K_mag_test , X_offset_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(target, pred, c=\"g\", alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"target\")\n",
    "plt.ylabel(\"pred\")\n",
    "\n",
    "#plt.ylim(0,100)\n",
    "#plt.xlim(0,100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
